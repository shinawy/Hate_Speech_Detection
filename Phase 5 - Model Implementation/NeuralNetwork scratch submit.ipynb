{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9fc4fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "### Don't mind about this \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7aa5de9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31011"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('freq_dict_sorted.txt', 'r') as freq_file:\n",
    "     freq_words= json.load(freq_file)\n",
    "freq_words\n",
    "all_words= freq_words.keys()\n",
    "len(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b6d4b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def create_dict_for_encoding(freq_dict, file_output):\n",
    "    encoded_dic = {}\n",
    "    index = 0\n",
    "    for key in freq_dict.keys():\n",
    "        encoded_dic[key]= index\n",
    "        index=index+1\n",
    "    \n",
    "    # writing the dict to a file for easier reading \n",
    "    json_dump= json.dumps(encoded_dic)\n",
    "    json_file= open(file_output, \"w\")\n",
    "    json_file.write(json_dump)\n",
    "    json_file.close()\n",
    "    \n",
    "    return encoded_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28a003ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31011"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_freq_words= create_dict_for_encoding(freq_words, \"freq_dic_encoded.txt\")\n",
    "len(encoded_freq_words.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352d6079",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8e145a16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocess_data</th>\n",
       "      <th>word_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>[yes, indeed, sort, remind, eld, lady, play, p...</td>\n",
       "      <td>{'yes': 1, 'indeed': 1, 'sort': 1, 'remind': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>[trans, woman, read, tweet, right, beautiful]</td>\n",
       "      <td>{'trans': 1, 'woman': 1, 'read': 1, 'tweet': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>[question, broad, criticize, america, country,...</td>\n",
       "      <td>{'question': 1, 'broad': 1, 'criticize': 1, 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>[time, illegal, go, back, country, origin, kee...</td>\n",
       "      <td>{'time': 1, 'illegal': 1, 'go': 1, 'back': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>[starter, bend, one, pink, kick, ass, pussy, g...</td>\n",
       "      <td>{'starter': 1, 'bend': 1, 'one': 1, 'pink': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135551</th>\n",
       "      <td>135551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ÿπÿßÿ¨ŸÑ ÿ≥ŸÖÿßÿ≠ÿ© #ÿßŸÑÿ≥ŸäÿØ_ÿπÿ®ÿØÿßŸÑŸÖŸÑŸÉ_ÿ®ÿØÿ±ÿßŸÑÿØŸäŸÜ_ÿßŸÑÿ≠Ÿàÿ´Ÿä  ŸÜÿµ...</td>\n",
       "      <td>[break, news, sayye, abdulmalik, saudi, regime...</td>\n",
       "      <td>{'break': 1, 'news': 1, 'sayye': 1, 'abdulmali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135552</th>\n",
       "      <td>135552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Millions of #Yemen-is participated in mass ral...</td>\n",
       "      <td>[million, yemen, participate, mass, rally, squ...</td>\n",
       "      <td>{'million': 1, 'yemen': 1, 'participate': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135553</th>\n",
       "      <td>135553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@AbeShinzo @realDonaldTrump @shinzoabe Áã¨Ë£ÅËÄÖ„ÅØË°å„Åç„Åæ...</td>\n",
       "      <td>[dictator, go, people, iran, stay, nodealterro...</td>\n",
       "      <td>{'dictator': 1, 'go': 1, 'people': 1, 'iran': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135554</th>\n",
       "      <td>135554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Millions of #Yemen-is participated in mass ral...</td>\n",
       "      <td>[million, yemen, participate, mass, rally, squ...</td>\n",
       "      <td>{'million': 1, 'yemen': 1, 'participate': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135555</th>\n",
       "      <td>135555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ŸÑÿß ÿ™ÿ™ÿ¥ŸÖÿ™ ÿßŸÑÿ±ÿ¨ÿßŸÑ ŸÖÿ≥ŸÉŸäŸÜ ŸäÿπÿßŸÜŸä ŸÉÿ≥ ÿßŸÖŸá üòÇ. ŸäŸÇŸàŸÑ ŸäÿßŸÑ...</td>\n",
       "      <td>[op, really, hope, commit, suicide, one, day, ...</td>\n",
       "      <td>{'op': 1, 'really': 1, 'hope': 1, 'commit': 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135556 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  hatespeech  \\\n",
       "0                0         0.0   \n",
       "1                1         0.0   \n",
       "2                2         1.0   \n",
       "3                3         0.0   \n",
       "4                4         1.0   \n",
       "...            ...         ...   \n",
       "135551      135551         0.0   \n",
       "135552      135552         0.0   \n",
       "135553      135553         0.0   \n",
       "135554      135554         0.0   \n",
       "135555      135555         1.0   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Yes indeed. She sort of reminds me of the elde...   \n",
       "1       The trans women reading this tweet right now i...   \n",
       "2       Question: These 4 broads who criticize America...   \n",
       "3       It is about time for all illegals to go back t...   \n",
       "4       For starters bend over the one in pink and kic...   \n",
       "...                                                   ...   \n",
       "135551  ÿπÿßÿ¨ŸÑ ÿ≥ŸÖÿßÿ≠ÿ© #ÿßŸÑÿ≥ŸäÿØ_ÿπÿ®ÿØÿßŸÑŸÖŸÑŸÉ_ÿ®ÿØÿ±ÿßŸÑÿØŸäŸÜ_ÿßŸÑÿ≠Ÿàÿ´Ÿä  ŸÜÿµ...   \n",
       "135552  Millions of #Yemen-is participated in mass ral...   \n",
       "135553  @AbeShinzo @realDonaldTrump @shinzoabe Áã¨Ë£ÅËÄÖ„ÅØË°å„Åç„Åæ...   \n",
       "135554  Millions of #Yemen-is participated in mass ral...   \n",
       "135555  ŸÑÿß ÿ™ÿ™ÿ¥ŸÖÿ™ ÿßŸÑÿ±ÿ¨ÿßŸÑ ŸÖÿ≥ŸÉŸäŸÜ ŸäÿπÿßŸÜŸä ŸÉÿ≥ ÿßŸÖŸá üòÇ. ŸäŸÇŸàŸÑ ŸäÿßŸÑ...   \n",
       "\n",
       "                                          preprocess_data  \\\n",
       "0       [yes, indeed, sort, remind, eld, lady, play, p...   \n",
       "1           [trans, woman, read, tweet, right, beautiful]   \n",
       "2       [question, broad, criticize, america, country,...   \n",
       "3       [time, illegal, go, back, country, origin, kee...   \n",
       "4       [starter, bend, one, pink, kick, ass, pussy, g...   \n",
       "...                                                   ...   \n",
       "135551  [break, news, sayye, abdulmalik, saudi, regime...   \n",
       "135552  [million, yemen, participate, mass, rally, squ...   \n",
       "135553  [dictator, go, people, iran, stay, nodealterro...   \n",
       "135554  [million, yemen, participate, mass, rally, squ...   \n",
       "135555  [op, really, hope, commit, suicide, one, day, ...   \n",
       "\n",
       "                                               word_tfidf  \n",
       "0       {'yes': 1, 'indeed': 1, 'sort': 1, 'remind': 1...  \n",
       "1       {'trans': 1, 'woman': 1, 'read': 1, 'tweet': 1...  \n",
       "2       {'question': 1, 'broad': 1, 'criticize': 1, 'a...  \n",
       "3       {'time': 1, 'illegal': 1, 'go': 1, 'back': 1, ...  \n",
       "4       {'starter': 1, 'bend': 1, 'one': 1, 'pink': 1,...  \n",
       "...                                                   ...  \n",
       "135551  {'break': 1, 'news': 1, 'sayye': 1, 'abdulmali...  \n",
       "135552  {'million': 1, 'yemen': 1, 'participate': 1, '...  \n",
       "135553  {'dictator': 1, 'go': 1, 'people': 1, 'iran': ...  \n",
       "135554  {'million': 1, 'yemen': 1, 'participate': 1, '...  \n",
       "135555  {'op': 1, 'really': 1, 'hope': 1, 'commit': 1,...  \n",
       "\n",
       "[135556 rows x 5 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"./final_cleaned_data.csv\", converters={\"preprocess_data\": eval})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "71ce2ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4052347369352887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "54932"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(df[df['hatespeech']==1])/ (len(df[df['hatespeech']==1]) + len(df[df['hatespeech']==0])))\n",
    "len(df[df['hatespeech']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d33829c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80624"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df[df['hatespeech']==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0b4ae834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.preprocess_data, df.hatespeech, test_size = 0.20, random_state = 25)\n",
    "# X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size = 0.10, random_state = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ad098bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4091915019179699\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "11094"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_test[y_test==1])/ (len(y_test[y_test==1]) + len(y_test[y_test==0])))\n",
    "len(y_test[y_test==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "6bb774cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108444,)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "c48e2c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4042455092029066\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43838"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(y_train[y_train==1])/ (len(y_train[y_train==1]) + len(y_train[y_train==0])))\n",
    "len(y_train[y_train==1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eada464",
   "metadata": {},
   "source": [
    "# Loading the encoded dict for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "885588a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " 'people': 1,\n",
       " 'woman': 2,\n",
       " 'url': 3,\n",
       " 'fuck': 4,\n",
       " 'get': 5,\n",
       " 'like': 6,\n",
       " 'white': 7,\n",
       " 'go': 8,\n",
       " 'man': 9,\n",
       " 'fucking': 10,\n",
       " 'nigger': 11,\n",
       " 'black': 12,\n",
       " 'hate': 13,\n",
       " 'bitch': 14,\n",
       " 'one': 15,\n",
       " 'make': 16,\n",
       " 'kill': 17,\n",
       " 'right': 18,\n",
       " 'know': 19,\n",
       " 'need': 20,\n",
       " 'see': 21,\n",
       " 'want': 22,\n",
       " 'country': 23,\n",
       " 'look': 24,\n",
       " 'back': 25,\n",
       " 'even': 26,\n",
       " 'gay': 27,\n",
       " 'say': 28,\n",
       " 'human': 29,\n",
       " 'would': 30,\n",
       " 'stupid': 31,\n",
       " 'muslim': 32,\n",
       " 'every': 33,\n",
       " 'think': 34,\n",
       " 'u': 35,\n",
       " 'take': 36,\n",
       " 'die': 37,\n",
       " 'come': 38,\n",
       " 'ass': 39,\n",
       " 'love': 40,\n",
       " 'world': 41,\n",
       " 'shit': 42,\n",
       " 'give': 43,\n",
       " 'muslims': 44,\n",
       " 'time': 45,\n",
       " 'america': 46,\n",
       " 'pussy': 47,\n",
       " 'girl': 48,\n",
       " 'god': 49,\n",
       " 'child': 50,\n",
       " 'good': 51,\n",
       " 'call': 52,\n",
       " 'way': 53,\n",
       " 'many': 54,\n",
       " 'well': 55,\n",
       " 'send': 56,\n",
       " 'feel': 57,\n",
       " 'stop': 58,\n",
       " 'racist': 59,\n",
       " 'illegal': 60,\n",
       " 'christian': 61,\n",
       " 'leave': 62,\n",
       " 'dick': 63,\n",
       " 'culture': 64,\n",
       " 'really': 65,\n",
       " 'work': 66,\n",
       " 'trump': 67,\n",
       " 'nigga': 68,\n",
       " 'use': 69,\n",
       " 'do': 70,\n",
       " 'we': 71,\n",
       " 'year': 72,\n",
       " 'religion': 73,\n",
       " 'day': 74,\n",
       " 'immigrant': 75,\n",
       " 'thing': 76,\n",
       " 'guy': 77,\n",
       " 'keep': 78,\n",
       " 'trans': 79,\n",
       " 'suck': 80,\n",
       " 'hope': 81,\n",
       " 'american': 82,\n",
       " 'rape': 83,\n",
       " 'shoot': 84,\n",
       " 'always': 85,\n",
       " 'much': 86,\n",
       " 'try': 87,\n",
       " 'life': 88,\n",
       " 'actually': 89,\n",
       " 'live': 90,\n",
       " 'help': 91,\n",
       " 'wish': 92,\n",
       " 'jews': 93,\n",
       " 'friend': 94,\n",
       " 'support': 95,\n",
       " 'not': 96,\n",
       " 'everyone': 97,\n",
       " 'jew': 98,\n",
       " 'bring': 99,\n",
       " 'proud': 100,\n",
       " 'show': 101,\n",
       " 'pride': 102,\n",
       " 'person': 103,\n",
       " 'another': 104,\n",
       " 'anti': 105,\n",
       " 'sex': 106,\n",
       " 'less': 107,\n",
       " 'cock': 108,\n",
       " 'to': 109,\n",
       " 'nothing': 110,\n",
       " 'still': 111,\n",
       " 'scum': 112,\n",
       " 'eat': 113,\n",
       " 'deport': 114,\n",
       " 'peace': 115,\n",
       " 'let': 116,\n",
       " 'family': 117,\n",
       " 'never': 118,\n",
       " 'ur': 119,\n",
       " 'anyone': 120,\n",
       " 'lot': 121,\n",
       " 'faggot': 122,\n",
       " 'going': 123,\n",
       " 'trash': 124,\n",
       " 'other': 125,\n",
       " 'female': 126,\n",
       " 'also': 127,\n",
       " 'real': 128,\n",
       " 'tell': 129,\n",
       " 'savage': 130,\n",
       " 'face': 131,\n",
       " 'talk': 132,\n",
       " 'either': 133,\n",
       " 'act': 134,\n",
       " 'mean': 135,\n",
       " 'job': 136,\n",
       " 'find': 137,\n",
       " 'change': 138,\n",
       " 'fact': 139,\n",
       " 'mexican': 140,\n",
       " 'name': 141,\n",
       " 'losangele': 142,\n",
       " 'great': 143,\n",
       " 'bunch': 144,\n",
       " 'deserve': 145,\n",
       " 'islam': 146,\n",
       " 'bear': 147,\n",
       " 'kid': 148,\n",
       " 'hole': 149,\n",
       " 'yeah': 150,\n",
       " 'force': 151,\n",
       " 'hand': 152,\n",
       " 'lol': 153,\n",
       " 'americans': 154,\n",
       " 'niggas': 155,\n",
       " 'belief': 156,\n",
       " 'dirty': 157,\n",
       " 'retard': 158,\n",
       " 'israel': 159,\n",
       " 'thank': 160,\n",
       " 'disgusting': 161,\n",
       " 'happy': 162,\n",
       " 'bad': 163,\n",
       " 'cunt': 164,\n",
       " 'terrorist': 165,\n",
       " 'christ': 166,\n",
       " 'violence': 167,\n",
       " 'funny': 168,\n",
       " 'idiot': 169,\n",
       " 'must': 170,\n",
       " 'gun': 171,\n",
       " 'little': 172,\n",
       " 'hard': 173,\n",
       " 'okay': 174,\n",
       " 'problem': 175,\n",
       " 'happen': 176,\n",
       " 'dumb': 177,\n",
       " 'care': 178,\n",
       " 'commit': 179,\n",
       " 'almost': 180,\n",
       " 'part': 181,\n",
       " 'male': 182,\n",
       " 'anything': 183,\n",
       " 'please': 184,\n",
       " 'I': 185,\n",
       " 'new': 186,\n",
       " 'something': 187,\n",
       " 'illegally': 188,\n",
       " 'could': 189,\n",
       " 'yes': 190,\n",
       " 'put': 191,\n",
       " 'hell': 192,\n",
       " 'big': 193,\n",
       " 'watch': 194,\n",
       " 'asian': 195,\n",
       " 'start': 196,\n",
       " 'believe': 197,\n",
       " 'point': 198,\n",
       " 'fight': 199,\n",
       " 'respect': 200,\n",
       " 'round': 201,\n",
       " 'may': 202,\n",
       " 'home': 203,\n",
       " 'oh': 204,\n",
       " 'since': 205,\n",
       " 'excuse': 206,\n",
       " 'allah': 207,\n",
       " 'first': 208,\n",
       " 'sure': 209,\n",
       " 'prick': 210,\n",
       " 'semite': 211,\n",
       " 'someone': 212,\n",
       " 'beat': 213,\n",
       " 'land': 214,\n",
       " 'death': 215,\n",
       " 'become': 216,\n",
       " 'whore': 217,\n",
       " 'true': 218,\n",
       " 'breed': 219,\n",
       " 'barbaric': 220,\n",
       " 'research': 221,\n",
       " 'assimilate': 222,\n",
       " 'next': 223,\n",
       " 'chink': 224,\n",
       " 'usa': 225,\n",
       " 'non': 226,\n",
       " 'asshole': 227,\n",
       " 'month': 228,\n",
       " 'steal': 229,\n",
       " 'nation': 230,\n",
       " 'monkey': 231,\n",
       " 'else': 232,\n",
       " 'ever': 233,\n",
       " 'attack': 234,\n",
       " 'm': 235,\n",
       " 'color': 236,\n",
       " 'seem': 237,\n",
       " 'safe': 238,\n",
       " 'jewish': 239,\n",
       " 'straight': 240,\n",
       " 'boy': 241,\n",
       " 'might': 242,\n",
       " 'power': 243,\n",
       " 'shut': 244,\n",
       " 'society': 245,\n",
       " 'middle': 246,\n",
       " 'kind': 247,\n",
       " 'sick': 248,\n",
       " 'incel': 249,\n",
       " 'r': 250,\n",
       " 'reward': 251,\n",
       " 'reject': 252,\n",
       " 'cum': 253,\n",
       " 'mexico': 254,\n",
       " 'community': 255,\n",
       " 'holy': 256,\n",
       " 'blow': 257,\n",
       " 'statement': 258,\n",
       " 'race': 259,\n",
       " 'guess': 260,\n",
       " 'president': 261,\n",
       " 'political': 262,\n",
       " 'long': 263,\n",
       " 'skin': 264,\n",
       " 'old': 265,\n",
       " 'hear': 266,\n",
       " 'racism': 267,\n",
       " 'lady': 268,\n",
       " 'history': 269,\n",
       " 'build': 270,\n",
       " 'deny': 271,\n",
       " 'brother': 272,\n",
       " 'especially': 273,\n",
       " 'hey': 274,\n",
       " 'half': 275,\n",
       " 'experience': 276,\n",
       " 'india': 277,\n",
       " 'catholic': 278,\n",
       " 'brown': 279,\n",
       " 'islamic': 280,\n",
       " 'low': 281,\n",
       " 'reason': 282,\n",
       " 'earth': 283,\n",
       " 'us': 284,\n",
       " 'east': 285,\n",
       " 'away': 286,\n",
       " 'etc': 287,\n",
       " 'liberal': 288,\n",
       " 'around': 289,\n",
       " 'threaten': 290,\n",
       " 'parent': 291,\n",
       " 's': 292,\n",
       " 'case': 293,\n",
       " 'idea': 294,\n",
       " 'source': 295,\n",
       " 'wall': 296,\n",
       " 'mad': 297,\n",
       " 'ppl': 298,\n",
       " 'street': 299,\n",
       " 'father': 300,\n",
       " 'al': 301,\n",
       " 'read': 302,\n",
       " 'without': 303,\n",
       " 'maybe': 304,\n",
       " 'bang': 305,\n",
       " 'refugee': 306,\n",
       " 'cause': 307,\n",
       " 'whatever': 308,\n",
       " 'ya': 309,\n",
       " 'kike': 310,\n",
       " 'son': 311,\n",
       " 'fun': 312,\n",
       " 'bullshit': 313,\n",
       " 'place': 314,\n",
       " 'hoe': 315,\n",
       " 'enjoy': 316,\n",
       " 'wonder': 317,\n",
       " 'process': 318,\n",
       " 'nasty': 319,\n",
       " 'baby': 320,\n",
       " 'innocent': 321,\n",
       " 'bomb': 322,\n",
       " 'course': 323,\n",
       " 'sub': 324,\n",
       " 'kick': 325,\n",
       " 'dude': 326,\n",
       " 'indian': 327,\n",
       " 'cry': 328,\n",
       " 'sorry': 329,\n",
       " 'close': 330,\n",
       " 'b': 331,\n",
       " 'wetback': 332,\n",
       " 'everything': 333,\n",
       " 'pig': 334,\n",
       " 'omar': 335,\n",
       " 'property': 336,\n",
       " 'united': 337,\n",
       " 'police': 338,\n",
       " 'arab': 339,\n",
       " 'word': 340,\n",
       " 'burn': 341,\n",
       " 'dog': 342,\n",
       " 'found': 343,\n",
       " 'ready': 344,\n",
       " 'gender': 345,\n",
       " 'slut': 346,\n",
       " 'form': 347,\n",
       " 'free': 348,\n",
       " 'rather': 349,\n",
       " 'ask': 350,\n",
       " 'plan': 351,\n",
       " 'comedian': 352,\n",
       " 'hang': 353,\n",
       " 'jesus': 354,\n",
       " 'church': 355,\n",
       " 'invade': 356,\n",
       " 'top': 357,\n",
       " 'none': 358,\n",
       " 'iran': 359,\n",
       " 'amazing': 360,\n",
       " 'rapist': 361,\n",
       " 'attention': 362,\n",
       " 'probably': 363,\n",
       " 'western': 364,\n",
       " 'realize': 365,\n",
       " 'blood': 366,\n",
       " 'insult': 367,\n",
       " 'states': 368,\n",
       " 'filthy': 369,\n",
       " 'head': 370,\n",
       " 'can': 371,\n",
       " 'shove': 372,\n",
       " 'chance': 373,\n",
       " 'planet': 374,\n",
       " 'border': 375,\n",
       " 'ugly': 376,\n",
       " 'wrong': 377,\n",
       " 'fine': 378,\n",
       " 'f': 379,\n",
       " 'bastard': 380,\n",
       " 'fag': 381,\n",
       " 'choke': 382,\n",
       " 'cop': 383,\n",
       " 'rid': 384,\n",
       " 'food': 385,\n",
       " 'level': 386,\n",
       " 'money': 387,\n",
       " 'end': 388,\n",
       " 'claim': 389,\n",
       " 'lgbt': 390,\n",
       " 'laugh': 391,\n",
       " 'hitler': 392,\n",
       " 'stage': 393,\n",
       " 'west': 394,\n",
       " 'turn': 395,\n",
       " 'fat': 396,\n",
       " 'enough': 397,\n",
       " 'piece': 398,\n",
       " 'play': 399,\n",
       " 'throw': 400,\n",
       " 'crazy': 401,\n",
       " 'murder': 402,\n",
       " 'pay': 403,\n",
       " 'tran': 404,\n",
       " 'matter': 405,\n",
       " 'moral': 406,\n",
       " 'wear': 407,\n",
       " 'light': 408,\n",
       " 'military': 409,\n",
       " 'assault': 410,\n",
       " 'lick': 411,\n",
       " 'focus': 412,\n",
       " 'huge': 413,\n",
       " 'learn': 414,\n",
       " 'state': 415,\n",
       " 'obama': 416,\n",
       " 'wipe': 417,\n",
       " 'return': 418,\n",
       " 'sort': 419,\n",
       " 'load': 420,\n",
       " 'slaughter': 421,\n",
       " 'hurt': 422,\n",
       " 'crap': 423,\n",
       " 'article': 424,\n",
       " 'joke': 425,\n",
       " 'somebody': 426,\n",
       " 'personal': 427,\n",
       " 'disgust': 428,\n",
       " 'palestinian': 429,\n",
       " 'post': 430,\n",
       " 'speak': 431,\n",
       " 'be': 432,\n",
       " 'rt': 433,\n",
       " 'killer': 434,\n",
       " 'red': 435,\n",
       " 'holocaust': 436,\n",
       " 'law': 437,\n",
       " 'transgender': 438,\n",
       " 'invader': 439,\n",
       " 'bi': 440,\n",
       " 'add': 441,\n",
       " 'definitely': 442,\n",
       " 'cancer': 443,\n",
       " 'pretty': 444,\n",
       " 'false': 445,\n",
       " 'war': 446,\n",
       " 'tear': 447,\n",
       " 'terror': 448,\n",
       " 'cheat': 449,\n",
       " 'murderer': 450,\n",
       " 'mubarak': 451,\n",
       " 'ship': 452,\n",
       " 'minority': 453,\n",
       " 'understand': 454,\n",
       " 'refer': 455,\n",
       " 'communist': 456,\n",
       " 'peaceful': 457,\n",
       " 'plenty': 458,\n",
       " 'doubt': 459,\n",
       " 'mother': 460,\n",
       " 'two': 461,\n",
       " 'already': 462,\n",
       " 'texas': 463,\n",
       " 'huh': 464,\n",
       " 'africans': 465,\n",
       " 'suicide': 466,\n",
       " 'hold': 467,\n",
       " 'got': 468,\n",
       " 'democracy': 469,\n",
       " 'religious': 470,\n",
       " 'here': 471,\n",
       " 'whole': 472,\n",
       " 'interesting': 473,\n",
       " 'constantly': 474,\n",
       " 'lesbian': 475,\n",
       " 'too': 476,\n",
       " 'abortion': 477,\n",
       " 'literally': 478,\n",
       " 'lazy': 479,\n",
       " 'creature': 480,\n",
       " 'quick': 481,\n",
       " 'cold': 482,\n",
       " 'killing': 483,\n",
       " 'risk': 484,\n",
       " 'finish': 485,\n",
       " 'hire': 486,\n",
       " 'mouth': 487,\n",
       " 'out': 488,\n",
       " 'rights': 489,\n",
       " 'tranny': 490,\n",
       " 'group': 491,\n",
       " 'condemn': 492,\n",
       " 'armed': 493,\n",
       " 'crime': 494,\n",
       " 'suit': 495,\n",
       " 'jealous': 496,\n",
       " 'seed': 497,\n",
       " 'wait': 498,\n",
       " 'anyway': 499,\n",
       " 'erase': 500,\n",
       " 'genocide': 501,\n",
       " 'useful': 502,\n",
       " 'minded': 503,\n",
       " 'destroy': 504,\n",
       " 'fully': 505,\n",
       " 'secular': 506,\n",
       " 'ion': 507,\n",
       " 'accept': 508,\n",
       " 'though': 509,\n",
       " 'backwards': 510,\n",
       " 'healthcare': 511,\n",
       " 'clue': 512,\n",
       " 'evidence': 513,\n",
       " 'cute': 514,\n",
       " 'unarmed': 515,\n",
       " 'issue': 516,\n",
       " 'ethic': 517,\n",
       " 'different': 518,\n",
       " 'desire': 519,\n",
       " 'angry': 520,\n",
       " 'nice': 521,\n",
       " 'shalom': 522,\n",
       " 'syrian': 523,\n",
       " 'resist': 524,\n",
       " 'concept': 525,\n",
       " 'pakistani': 526,\n",
       " 'full': 527,\n",
       " 'bagel': 528,\n",
       " 'author': 529,\n",
       " 'run': 530,\n",
       " 'dis': 531,\n",
       " 'wanna': 532,\n",
       " 'spic': 533,\n",
       " 'you': 534,\n",
       " 'synagogue': 535,\n",
       " 'stand': 536,\n",
       " 'dickhead': 537,\n",
       " 'parasite': 538,\n",
       " 'video': 539,\n",
       " 'young': 540,\n",
       " 'african': 541,\n",
       " 'reminder': 542,\n",
       " 'pal': 543,\n",
       " 'morality': 544,\n",
       " 'small': 545,\n",
       " 'now': 546,\n",
       " 'dysphoria': 547,\n",
       " 'share': 548,\n",
       " 'imply': 549,\n",
       " 'lie': 550,\n",
       " 'confront': 551,\n",
       " 'n': 552,\n",
       " 'independence': 553,\n",
       " 'blacks': 554,\n",
       " 'unacceptable': 555,\n",
       " 'nobody': 556,\n",
       " 'challenge': 557,\n",
       " 'whatsoever': 558,\n",
       " 'extinct': 559,\n",
       " 'intellectual': 560,\n",
       " 'strap': 561,\n",
       " 'honour': 562,\n",
       " 'bulldoze': 563,\n",
       " 'chinatown': 564,\n",
       " 'spick': 565,\n",
       " 'evil': 566,\n",
       " 'six': 567,\n",
       " 'towelhead': 568,\n",
       " 'gauche': 569,\n",
       " 'niggrus': 570,\n",
       " 'americanus': 571,\n",
       " 'migrant': 572,\n",
       " 'cole': 573,\n",
       " 'yesterday': 574,\n",
       " 'guatemala': 575,\n",
       " 'animal': 576,\n",
       " 'determine': 577,\n",
       " 'slant': 578,\n",
       " 'ebloa': 579,\n",
       " 'loser': 580,\n",
       " 'gook': 581,\n",
       " 'cockroach': 582,\n",
       " 'rashid': 583,\n",
       " 'feeble': 584,\n",
       " 'comment': 585,\n",
       " 'tribe': 586,\n",
       " 'natural': 587,\n",
       " 'technology': 588,\n",
       " 'prepared': 589,\n",
       " 'open': 590,\n",
       " 'babylonian': 591,\n",
       " 'hateful': 592,\n",
       " 'prosperity': 593,\n",
       " 'mong': 594,\n",
       " 'yey': 595,\n",
       " 'last': 596,\n",
       " 'vampire': 597,\n",
       " 'serpent': 598,\n",
       " 'mom': 599,\n",
       " 'whitepower': 600,\n",
       " 'apart': 601,\n",
       " 'declaration': 602,\n",
       " 'subversive': 603,\n",
       " 'http': 604,\n",
       " 'signatures': 605,\n",
       " 'kenite': 606,\n",
       " 'posterity': 607,\n",
       " 'graduation': 608,\n",
       " 'lgbtq': 609,\n",
       " 'nonbinary': 610,\n",
       " 'exact': 611,\n",
       " 'triple': 612,\n",
       " 'term': 613,\n",
       " 'prosecute': 614,\n",
       " 'appetite': 615,\n",
       " 'body': 616,\n",
       " 'grow': 617,\n",
       " 'handicapped': 618,\n",
       " 'disadvantage': 619,\n",
       " 'bible': 620,\n",
       " 'happiness': 621,\n",
       " 'sound': 622,\n",
       " 'genuine': 623,\n",
       " 'perpetuate': 624,\n",
       " 'mn': 625,\n",
       " 'being': 626,\n",
       " 'pangolin': 627,\n",
       " 'contribution': 628,\n",
       " 'perceive': 629,\n",
       " 'camera': 630,\n",
       " 'aware': 631,\n",
       " 'government': 632,\n",
       " 'ayo': 633,\n",
       " 'mpls': 634,\n",
       " 'trannie': 635,\n",
       " 'scalp': 636,\n",
       " 'pointless': 637,\n",
       " 'barr': 638,\n",
       " 'equality': 639,\n",
       " 'season': 640,\n",
       " 'crippled': 641,\n",
       " 'authoritarian': 642,\n",
       " 'nil': 643,\n",
       " 'lmao': 644,\n",
       " 'splash': 645,\n",
       " 'today': 646,\n",
       " 'retarded': 647,\n",
       " 'politician': 648,\n",
       " 'positive': 649,\n",
       " 'lailat': 650,\n",
       " 'miraj': 651,\n",
       " 'grateful': 652,\n",
       " 'mine': 653,\n",
       " 'camp': 654,\n",
       " 'hollow': 655,\n",
       " 'damn': 656,\n",
       " 'ghanaian': 657,\n",
       " 'high': 658,\n",
       " 'self': 659,\n",
       " 'stay': 660,\n",
       " 'school': 661,\n",
       " 'immense': 662,\n",
       " 'outcast': 663,\n",
       " 'laud': 664,\n",
       " 'verge': 665,\n",
       " 'date': 666,\n",
       " 'protect': 667,\n",
       " 'twibbon': 668,\n",
       " 'dedicated': 669,\n",
       " 'nugs': 670,\n",
       " 'progess': 671,\n",
       " 'yet': 672,\n",
       " 'least': 673,\n",
       " 'beautiful': 674,\n",
       " 'move': 675,\n",
       " 'agree': 676,\n",
       " 'treat': 677,\n",
       " 'surely': 678,\n",
       " 'yt': 679,\n",
       " 'blink': 680,\n",
       " 'igbo': 681,\n",
       " 'instead': 682,\n",
       " 'heart': 683,\n",
       " 'attach': 684,\n",
       " 'million': 685,\n",
       " 'step': 686,\n",
       " 'bull': 687,\n",
       " 'boi': 688,\n",
       " 'wife': 689,\n",
       " 'thinking': 690,\n",
       " 'transvestigation': 691,\n",
       " 'spend': 692,\n",
       " 'abeg': 693,\n",
       " 'jesuit': 694,\n",
       " 'ogbakoumuigboday': 695,\n",
       " 'control': 696,\n",
       " 'ignore': 697,\n",
       " 'prove': 698,\n",
       " 'language': 699,\n",
       " 'custody': 700,\n",
       " 'lose': 701,\n",
       " 'break': 702,\n",
       " 'allow': 703,\n",
       " 'citizen': 704,\n",
       " 'geez': 705,\n",
       " 'devil': 706,\n",
       " 'fake': 707,\n",
       " 'barackobama': 708,\n",
       " 'defend': 709,\n",
       " 'existence': 710,\n",
       " 'northkorea': 711,\n",
       " 'bet': 712,\n",
       " 'likely': 713,\n",
       " 'useless': 714,\n",
       " 'pakistan': 715,\n",
       " 'careful': 716,\n",
       " 'zionist': 717,\n",
       " 'undesirable': 718,\n",
       " 'mind': 719,\n",
       " 'legal': 720,\n",
       " 'type': 721,\n",
       " 'hot': 722,\n",
       " 'raping': 723,\n",
       " 'prison': 724,\n",
       " 'typical': 725,\n",
       " 'threat': 726,\n",
       " 'amongst': 727,\n",
       " 'poor': 728,\n",
       " 'far': 729,\n",
       " 'dangerous': 730,\n",
       " 'follow': 731,\n",
       " 'queer': 732,\n",
       " 'whimper': 733,\n",
       " 'meet': 734,\n",
       " 'squeal': 735,\n",
       " 'celebrate': 736,\n",
       " 'story': 737,\n",
       " 'profile': 738,\n",
       " 'nt': 739,\n",
       " 'ok': 740,\n",
       " 'imagine': 741,\n",
       " 'consider': 742,\n",
       " 'sexual': 743,\n",
       " 'worth': 744,\n",
       " 'question': 745,\n",
       " 'europe': 746,\n",
       " 'click': 747,\n",
       " 'side': 748,\n",
       " 'slave': 749,\n",
       " 'four': 750,\n",
       " 'rest': 751,\n",
       " 'meh': 752,\n",
       " 'exist': 753,\n",
       " 'nawaz': 754,\n",
       " 'standing': 755,\n",
       " 'due': 756,\n",
       " 'create': 757,\n",
       " 'centre': 758,\n",
       " 'maryam': 759,\n",
       " 'w': 760,\n",
       " 'cite': 761,\n",
       " 'saudi': 762,\n",
       " 'early': 763,\n",
       " 'arena': 764,\n",
       " 'fill': 765,\n",
       " 'notable': 766,\n",
       " 'relationship': 767,\n",
       " 'tie': 768,\n",
       " 'notwithstanding': 769,\n",
       " 'immigration': 770,\n",
       " 'marriyum': 771,\n",
       " 'aurangzeb': 772,\n",
       " 'sahiba': 773,\n",
       " 'pakistanstandswithmaryam': 774,\n",
       " 'grief': 775,\n",
       " 'easy': 776,\n",
       " 'wahh': 777,\n",
       " 'list': 778,\n",
       " 'comical': 779,\n",
       " 'feminist': 780,\n",
       " 'disprove': 781,\n",
       " 'dead': 782,\n",
       " 'scare': 783,\n",
       " 'tit': 784,\n",
       " 'chinese': 785,\n",
       " 'superior': 786,\n",
       " 'amurica': 787,\n",
       " 'biologically': 788,\n",
       " 'adapt': 789,\n",
       " 'vote': 790,\n",
       " 'gc': 791,\n",
       " 'remember': 792,\n",
       " 'pathetic': 793,\n",
       " 'supremacist': 794,\n",
       " 'entire': 795,\n",
       " 'hatesub': 796,\n",
       " 'missrepresent': 797,\n",
       " 'inherent': 798,\n",
       " 'separate': 799,\n",
       " 'folk': 800,\n",
       " 'that': 801,\n",
       " 'together': 802,\n",
       " 'hair': 803,\n",
       " 'fuckin': 804,\n",
       " 'several': 805,\n",
       " 'pray': 806,\n",
       " 'hindu': 807,\n",
       " 'deal': 808,\n",
       " 'single': 809,\n",
       " 'slavery': 810,\n",
       " 'train': 811,\n",
       " 'sad': 812,\n",
       " 'obvious': 813,\n",
       " 'mass': 814,\n",
       " 'base': 815,\n",
       " 'hit': 816,\n",
       " 'south': 817,\n",
       " 'strong': 818,\n",
       " 'isis': 819,\n",
       " 'christians': 820,\n",
       " 'ball': 821,\n",
       " 'sister': 822,\n",
       " 'soon': 823,\n",
       " 'pass': 824,\n",
       " 'africa': 825,\n",
       " 'news': 826,\n",
       " 'wow': 827,\n",
       " 'china': 828,\n",
       " 'house': 829,\n",
       " 'ten': 830,\n",
       " 'worst': 831,\n",
       " 'city': 832,\n",
       " 'include': 833,\n",
       " 'native': 834,\n",
       " 'honestly': 835,\n",
       " 'criminal': 836,\n",
       " 'marry': 837,\n",
       " 'supportive': 838,\n",
       " 'nazi': 839,\n",
       " 'deep': 840,\n",
       " 'european': 841,\n",
       " 'spread': 842,\n",
       " 'eye': 843,\n",
       " 'sexuality': 844,\n",
       " 'yo': 845,\n",
       " 'blame': 846,\n",
       " 'sense': 847,\n",
       " 'population': 848,\n",
       " 'raise': 849,\n",
       " 'cut': 850,\n",
       " 'moron': 851,\n",
       " 'park': 852,\n",
       " 'all': 853,\n",
       " 'important': 854,\n",
       " 'ban': 855,\n",
       " 'listen': 856,\n",
       " 'downvote': 857,\n",
       " 'faith': 858,\n",
       " 'bless': 859,\n",
       " 'continue': 860,\n",
       " 'able': 861,\n",
       " 'prime': 862,\n",
       " 'win': 863,\n",
       " 'welcome': 864,\n",
       " 'alone': 865,\n",
       " 'brain': 866,\n",
       " 'number': 867,\n",
       " 'view': 868,\n",
       " 'forget': 869,\n",
       " 'climate': 870,\n",
       " 'stick': 871,\n",
       " 'teach': 872,\n",
       " 'choose': 873,\n",
       " 'atrocity': 874,\n",
       " 'jail': 875,\n",
       " 'save': 876,\n",
       " 'stuff': 877,\n",
       " 'flag': 878,\n",
       " 'trailer': 879,\n",
       " 'behind': 880,\n",
       " 'jordan': 881,\n",
       " 'game': 882,\n",
       " 'leader': 883,\n",
       " 'sentiment': 884,\n",
       " 'exactly': 885,\n",
       " 'couple': 886,\n",
       " 'medium': 887,\n",
       " 'breast': 888,\n",
       " 'mexicans': 889,\n",
       " 'mentally': 890,\n",
       " 'audience': 891,\n",
       " 'prop': 892,\n",
       " 'worry': 893,\n",
       " 'presidency': 894,\n",
       " 'genocidal': 895,\n",
       " 'autism': 896,\n",
       " 'mental': 897,\n",
       " 'defender': 898,\n",
       " 'throat': 899,\n",
       " 'weak': 900,\n",
       " 'bit': 901,\n",
       " 'haircut': 902,\n",
       " 'hook': 903,\n",
       " 'till': 904,\n",
       " 'fire': 905,\n",
       " 'em': 906,\n",
       " 'expect': 907,\n",
       " 'ching': 908,\n",
       " 'victim': 909,\n",
       " 'write': 910,\n",
       " 'bisexual': 911,\n",
       " 'vietnam': 912,\n",
       " 'shame': 913,\n",
       " 'chong': 914,\n",
       " 'age': 915,\n",
       " 'cool': 916,\n",
       " 'ago': 917,\n",
       " 'second': 918,\n",
       " 'absolutely': 919,\n",
       " 'whereas': 920,\n",
       " 'gang': 921,\n",
       " 'team': 922,\n",
       " 'dehumanize': 923,\n",
       " 'harmless': 924,\n",
       " 'uk': 925,\n",
       " 'abuse': 926,\n",
       " 'virgin': 927,\n",
       " 'bro': 928,\n",
       " 'christianity': 929,\n",
       " 'fatass': 930,\n",
       " 'shithole': 931,\n",
       " 'bigfact': 932,\n",
       " 'cuck': 933,\n",
       " 'westbrook': 934,\n",
       " 'dramatic': 935,\n",
       " 'specialty': 936,\n",
       " 'fagot': 937,\n",
       " 'member': 938,\n",
       " 'atheist': 939,\n",
       " 'immorality': 940,\n",
       " 'acorn': 941,\n",
       " 'oboama': 942,\n",
       " 'finally': 943,\n",
       " 'pull': 944,\n",
       " 'completely': 945,\n",
       " 'identify': 946,\n",
       " 'testicular': 947,\n",
       " 'hispanic': 948,\n",
       " 'homophobic': 949,\n",
       " 'movie': 950,\n",
       " 'majority': 951,\n",
       " 'sit': 952,\n",
       " 'line': 953,\n",
       " 'hose': 954,\n",
       " 'mens': 955,\n",
       " 'prostate': 956,\n",
       " 'niggerloving': 957,\n",
       " 'inside': 958,\n",
       " 'cripple': 959,\n",
       " 'transgression': 960,\n",
       " 'ovarian': 961,\n",
       " 'lead': 962,\n",
       " 'unreal': 963,\n",
       " 'authoritarianism': 964,\n",
       " 'undeniable': 965,\n",
       " 'wake': 966,\n",
       " 'push': 967,\n",
       " 'truly': 968,\n",
       " 'weird': 969,\n",
       " 'identity': 970,\n",
       " 'action': 971,\n",
       " 'walk': 972,\n",
       " 'thousand': 973,\n",
       " 'sorrynotsorry': 974,\n",
       " 'globalist': 975,\n",
       " 'dunk': 976,\n",
       " 'pregnant': 977,\n",
       " 'drop': 978,\n",
       " 'ignorant': 979,\n",
       " 'freedom': 980,\n",
       " 'social': 981,\n",
       " 'sin': 982,\n",
       " 'future': 983,\n",
       " 'iranian': 984,\n",
       " 'past': 985,\n",
       " 'opinion': 986,\n",
       " 'failure': 987,\n",
       " 'wtf': 988,\n",
       " 'homosexual': 989,\n",
       " 'rule': 990,\n",
       " 'window': 991,\n",
       " 'truth': 992,\n",
       " 'decide': 993,\n",
       " 'waste': 994,\n",
       " 'getting': 995,\n",
       " 'equal': 996,\n",
       " 'chick': 997,\n",
       " 'ill': 998,\n",
       " 'marriage': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "with open('freq_dic_encoded.txt', 'r') as freq_file:\n",
    "     encoded_words= json.load(freq_file)\n",
    "encoded_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "0b9490be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class layer():\n",
    "    def __init__(self,n_input,n_output, name, activation=\"sigmoid\", weight_init= \"uniform\"):\n",
    "        self.n_input = n_input\n",
    "        self.n_output = n_output\n",
    "        self.activation = self.get_activations()[activation]\n",
    "        self.act_name = activation\n",
    "        \n",
    "        self.name= name\n",
    "\n",
    "        \n",
    "        # Trying different weight initializations\n",
    "        if weight_init == \"hnormal\":\n",
    "            self.W = np.random.randn(self.n_output,self.n_input)*np.sqrt(2/self.n_input)\n",
    "            self.b = np.random.randn(self.n_output,1)*np.sqrt(2/self.n_input)\n",
    "        elif weight_init == \"normal\":\n",
    "            self.W = np.random.randn(0, 1, (self.n_output,self.n_input))\n",
    "            self.b = np.random.randn(0, 1, (self.n_output,1))\n",
    "        elif weight_init == \"uniform\":\n",
    "            self.W = np.random.uniform(-0.05,0.05,(self.n_output,self.n_input))\n",
    "            self.b = np.random.uniform(-0.05,0.05,(self.n_output,1))\n",
    "            \n",
    "        \n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "        self.Z = None\n",
    "        self.X = None\n",
    "\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        return (1/(1+np.exp(-1*z)))\n",
    "\n",
    "    def _diff_sigmoid(self, z):\n",
    "        sig = self.sigmoid(z)\n",
    "        return sig * (1-sig)\n",
    "    \n",
    "    def _leaky_relu(self, z):\n",
    "        return np.maximum(z, -0.001 * z)\n",
    "    \n",
    "    def _diff_leaky_relu(self, z):\n",
    "        leaky_relu= self._leaky_relu(z)\n",
    "        return np.multiply(leaky_relu, np.ones_like(leaky_relu) - leaky_relu)\n",
    "    \n",
    "    # Hyperbolic Tangent (htan) Activation Function\n",
    "    def _tanh(self,x):\n",
    "        return(np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "\n",
    "    # htan derivative\n",
    "    def _diff_tanh(self,x):\n",
    "        return np.multiply(self._tanh(x), np.ones_like(self._tanh(x)) - self._tanh(x))\n",
    "\n",
    "\n",
    "    def get_activations(self):\n",
    "        return {\"sigmoid\":self.sigmoid, \"relu\": self._leaky_relu, \"tanh\": self._tanh}\n",
    "\n",
    "    def get_activations_diff(self):\n",
    "        return {\"sigmoid\":self._diff_sigmoid, \"relu\": self._diff_leaky_relu, \"tanh\": self._diff_tanh}\n",
    "\n",
    "\n",
    "    def zeroing_delta(self):\n",
    "        self.dW = np.zeros_like(self.W)\n",
    "        self.db = np.zeros_like(self.b)\n",
    "\n",
    "    def _set_target(self, t):\n",
    "        self.target = t\n",
    "\n",
    "    def forward(self,Ai):\n",
    "        z =  np.add(np.dot(self.W,Ai),self.b)\n",
    "\n",
    "        A = self.activation(z)\n",
    "       \n",
    "        \n",
    "        self.Z = z\n",
    "        self.Ai = Ai\n",
    "        return A\n",
    "\n",
    "    def backward(self,inp):\n",
    "        \n",
    "        act_diff = self.get_activations_diff()[self.act_name](self.Z)\n",
    "        \n",
    "#         tmp = inp * act_diff\n",
    "        tmp= np.multiply(inp,act_diff)\n",
    "\n",
    "#         bet = tmp @ self.Ai.T # vector of 1s\n",
    "        bet= np.dot(tmp, self.Ai.T);\n",
    "\n",
    "     \n",
    "        \n",
    "        e = np.ones((self.Ai.shape[1],1))\n",
    "        db = np.dot(tmp ,e)\n",
    "#         print(\"db shape: \",end='')\n",
    "#         print(db.shape)\n",
    "        \n",
    "        self.dW = self.dW + bet\n",
    "        self.db = self.db + db\n",
    "    \n",
    "        return np.dot(self.W.T , tmp)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "78171acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    \n",
    "    def __init__(self,all_words, lr=0.5):\n",
    "        self.layers = []\n",
    "        self.all_words = all_words\n",
    "        self.learning_rate=lr\n",
    "        \n",
    "\n",
    "     ## Binary Cross Entropy\n",
    "    def imp_binary_cross_log(self,y_true, y_pred):\n",
    "        y_logp= np.multiply(y_true, np.log(y_pred)) + np.multiply((np.ones_like(y_true)- y_true), np.log(np.ones_like (y_pred)- y_pred))\n",
    "#         print(\"y_logp shape:::{}, ytrue shape: {}, ypred shape: {}\".format(y_logp.shape, y_true.shape, y_pred.shape))\n",
    "        return y_logp\n",
    "\n",
    "    def imp_binary_cross_sum(self,y_true, y_pred):\n",
    "        y_logp= self.imp_binary_cross_log(y_true, y_pred)\n",
    "        return (-1/y_true.size)* y_logp.sum()\n",
    "    ####\n",
    "    \n",
    "    ## focal Loss \n",
    "    def imp_focal_loss_log(self, y_true, y_pred, alpha=0.25, gamma=2):\n",
    "        \n",
    "        y_true= y_true.reshape(y_true.shape[0], y_pred.shape[1])\n",
    "        comp_ytrue= (np.ones_like(y_true)- y_true)\n",
    "        comp_yhat= (np.ones_like(y_pred)- y_pred)\n",
    "        \n",
    "        first_term= alpha * np.multiply(np.multiply(y_true, np.log(y_pred)), comp_yhat**gamma)\n",
    "        second_term= (1- alpha) * np.multiply(np.multiply(comp_ytrue, np.log(comp_yhat)), y_pred**gamma)\n",
    "        \n",
    "        focal_loss= first_term + second_term\n",
    "#         print(\"y_logp focal_loss:::{} shape: {}, shape[0]: {}\".format(focal_loss[0], focal_loss.shape, focal_loss[0].shape))\n",
    "        return focal_loss\n",
    "    \n",
    "    def imp_focal_loss_sum(self, y_true, y_pred, alpha=0.25, gamma=2):\n",
    "        focal_loss= self.imp_focal_loss_log(y_true, y_pred,alpha, gamma)\n",
    "        return ((-1)/y_true.size)* (focal_loss.sum())\n",
    "   \n",
    "        \n",
    "        \n",
    "    def transform_words(self, x):\n",
    "        encoding = np.zeros(len(self.all_words))\n",
    "        i = 0\n",
    "        for word in x:\n",
    "            if word in self.all_words:\n",
    "                encoding[self.all_words[word]] = 1\n",
    "        return encoding.astype(np.int8)\n",
    "    \n",
    "    def add(self,layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    def forward(self,input_):\n",
    "        X_words = input_\n",
    "        for layer in self.layers:\n",
    "            X_words= layer.forward(X_words)\n",
    "\n",
    "\n",
    "        return X_words.T\n",
    "    \n",
    "    def backward(self,input_):\n",
    "        gd = input_\n",
    "        for layer in self.layers[::-1]:\n",
    "            gd = layer.backward(gd)\n",
    "\n",
    "            \n",
    "            \n",
    "    def zeroing(self):\n",
    "        for layer in self.layers:\n",
    "            layer.zeroing_delta()\n",
    "            \n",
    "\n",
    "    def batch(self,x,y,bs):\n",
    "        x = x.copy()\n",
    "        y = y.copy()\n",
    "        rem = x.shape[0] % bs\n",
    "\n",
    "        for i in range(0,x.shape[0],bs):\n",
    "            yield (x[i:i+bs],y[i:i+bs])\n",
    "        \n",
    "        if rem !=0:\n",
    "            yield (x[x.shape[0]-rem:],y[x.shape[0]-rem:] )\n",
    "\n",
    "    def fit(self,train_data, batch_size=32, epochs=5):\n",
    "        x_train= train_data[0]\n",
    "        y_train= train_data[1]\n",
    "        no_of_batches_train = np.ceil(x_train.shape[0]/batch_size)\n",
    "       \n",
    "        \n",
    "        \n",
    "        # Two moving averages as per ADAM optimizer concept\n",
    "        m = []\n",
    "        v = []\n",
    "        \n",
    "        count=0\n",
    "        for i in range(epochs):\n",
    "             \n",
    "            # initializing the moving averages as per ADAM optimizer concept\n",
    "            for layer in self.layers:\n",
    "                w = np.zeros_like(layer.W)\n",
    "                b = np.zeros_like(layer.b)\n",
    "                m.append([w,b])\n",
    "                v.append([w,b])\n",
    "                \n",
    "            print()\n",
    "            print(\"Epoch {}/{}\".format(i+1,epochs))\n",
    "            j=0\n",
    "            \n",
    "            data = self.batch(x_train,y_train,batch_size)\n",
    "            print(\"length of data is {}\".format(count))\n",
    "            losses = []\n",
    "            \n",
    "            for temp_x,temp_y in data:\n",
    "                count+=1\n",
    "                curr_x = temp_x.copy()\n",
    "                curr_y = temp_y.copy()\n",
    "                \n",
    "                word_encodings = []\n",
    "                for dic in curr_x:\n",
    "                    word_encodings.append(self.transform_words(dic))\n",
    "                curr_x = np.array(word_encodings)\n",
    "\n",
    "    \n",
    "                curr_x = curr_x.T\n",
    "                curr_y = curr_y.T\n",
    "                y_hat = self.forward(curr_x)            \n",
    "                \n",
    "#                 print(\"curr_y[0]: {}; y_hat[0]: {}\".format(curr_y[0], y_hat[0]))\n",
    "                dl_dyhat = self.imp_focal_loss_sum(curr_y,y_hat)\n",
    "\n",
    "                self.backward(1)\n",
    "                \n",
    "                \n",
    "                losses.append(dl_dyhat)\n",
    "                if j == no_of_batches_train-1:\n",
    "                    print(\"lossses: {}\".format(np.array(losses)))\n",
    "                    print(\"losses sum: {}\".format(sum(losses)))\n",
    "                    print(\"losses length: {}\".format(len(losses)))\n",
    "\n",
    "                    loss = sum(losses) / len(losses)\n",
    "                    print()\n",
    "                    print(\"loss: {}....\".format(loss))\n",
    "\n",
    "                if batch_size == 1:\n",
    "                    N = train_data[0].shape[0]\n",
    "                else:\n",
    "                    N = curr_x.shape[-1]\n",
    "                \n",
    "                #updating the weights using ADAM optimizer\n",
    "                beta1= 0.9\n",
    "                beta2= 0.999 #best values for betas as towardsdatascience artice discussed\n",
    "                for i in range(len(self.layers)):\n",
    "                    m[i][0] = beta1 * m[i][0] + (1-beta1) * self.layers[i].dW\n",
    "                    m[i][1] = beta1 * m[i][1] + (1-beta1) * self.layers[i].db\n",
    "\n",
    "                    v[i][0] = beta2 * v[i][0] + (1-beta2) * np.square(self.layers[i].dW)\n",
    "                    v[i][1] = beta2 * v[i][1] + (1-beta2) * np.square(self.layers[i].db)\n",
    "\n",
    "                    deltaW = (-1 * self.learning_rate * m[i][0]) / (np.sqrt(v[i][0] + 0.001))\n",
    "                    deltab = (-1 * self.learning_rate * m[i][1]) / (np.sqrt(v[i][1] + 0.001))\n",
    "\n",
    "                    self.layers[i].W = self.layers[i].W + deltaW/N\n",
    "                    self.layers[i].b = self.layers[i].b + deltab/N\n",
    "                self.zeroing()\n",
    "                j += 1 \n",
    "#                 print(\"Reached here\")\n",
    "            \n",
    "\n",
    "    def predict(self,data):\n",
    "        \n",
    "        word_encodings = []\n",
    "        for dic in data:\n",
    "            word_encodings.append(self.transform_words(dic))\n",
    "        words = np.array(word_encodings)\n",
    "\n",
    "        \n",
    "        y_hat= self.forward(words.T)\n",
    "        return y_hat.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "094827d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31011\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "37766bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "length of data is 0\n",
      "lossses: [0.34675805 0.34674979 0.34669888 0.3466746  0.34663982 0.34661788\n",
      " 0.34659136 0.34657934 0.34657367 0.3465767  0.3465876  0.34660998\n",
      " 0.34664346 0.34667877 0.34672352 0.34678463 0.34685123 0.34685238\n",
      " 0.34696832 0.34706103 0.34720705 0.34721993 0.34750093 0.34750579\n",
      " 0.34765734 0.34780075 0.34772298 0.34822059 0.34830138 0.34812773\n",
      " 0.34837808 0.34851727 0.34877888 0.34869641 0.34896154 0.3493326\n",
      " 0.34905701 0.34987552 0.34981809 0.34985343 0.350001   0.35033859\n",
      " 0.3503168  0.35043123 0.35125618 0.35128112 0.35147402 0.35143022\n",
      " 0.35204379 0.35252086 0.35212855 0.35216571 0.35224232 0.35247744\n",
      " 0.3523956  0.35418855 0.35420582 0.35363142 0.35296957 0.35427875\n",
      " 0.35367163 0.3539845  0.35466233 0.35523575 0.35435671 0.35566305\n",
      " 0.35714755 0.35694689 0.35632242 0.35654668 0.35779443 0.35578584\n",
      " 0.35843719 0.35746835 0.35754518 0.35926799 0.35935541 0.35984879\n",
      " 0.35872685 0.35835624 0.35827018 0.35956643 0.35892006 0.35935198\n",
      " 0.36067842 0.35979384 0.36072373 0.36095116 0.36221681 0.36064784\n",
      " 0.36172344 0.36230696 0.36103427 0.35918892 0.36150254 0.3607974\n",
      " 0.36371155 0.36123016 0.36212438 0.36201618 0.36193267 0.36400043\n",
      " 0.36390964 0.3637128  0.36335282 0.36127929 0.36184435 0.36306478\n",
      " 0.36224789]\n",
      "losses sum: 38.56375655782034\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.3537959317231224....\n",
      "\n",
      "Epoch 2/10\n",
      "length of data is 110\n",
      "lossses: [0.36143476 0.3630266  0.36121804 0.36267815 0.36227818 0.36439223\n",
      " 0.36162656 0.36411585 0.36409937 0.36068059 0.36036643 0.36180516\n",
      " 0.36251687 0.36149045 0.36123045 0.36155273 0.36166514 0.35844805\n",
      " 0.36024126 0.36060141 0.36180516 0.35988679 0.36328385 0.36119535\n",
      " 0.36186787 0.36190606 0.35949489 0.36355107 0.36295332 0.35998808\n",
      " 0.36093077 0.36085682 0.36175393 0.36044432 0.36093085 0.36223936\n",
      " 0.36001765 0.36342346 0.36241642 0.36176687 0.36172011 0.36227649\n",
      " 0.36177828 0.36164642 0.36417982 0.36352907 0.36352401 0.36262888\n",
      " 0.36403199 0.36488449 0.36313603 0.36266319 0.36242403 0.36260409\n",
      " 0.36170399 0.36582871 0.3653036  0.36350074 0.36135653 0.36388581\n",
      " 0.36213616 0.36222096 0.36350074 0.3640495  0.36181186 0.36424123\n",
      " 0.36659225 0.36582017 0.36415353 0.36426281 0.36571025 0.36196681\n",
      " 0.36599068 0.36442189 0.36366266 0.36643645 0.36622256 0.36662283\n",
      " 0.36447958 0.36359623 0.36334662 0.36448156 0.36347601 0.36374266\n",
      " 0.36550514 0.36380132 0.36474961 0.36505223 0.36614827 0.36420818\n",
      " 0.36540882 0.36575606 0.36413152 0.36155538 0.36429341 0.3632001\n",
      " 0.36678792 0.36343012 0.36460186 0.3640437  0.36424459 0.36641429\n",
      " 0.36649555 0.36562193 0.36513338 0.36279549 0.36344904 0.36489499\n",
      " 0.36382534]\n",
      "losses sum: 39.58728172132693\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.36318607083786175....\n",
      "\n",
      "Epoch 3/10\n",
      "length of data is 220\n",
      "lossses: [0.36287034 0.36453079 0.36239605 0.36411247 0.36341881 0.36591553\n",
      " 0.36270652 0.36532595 0.36535865 0.36173228 0.36120331 0.36290818\n",
      " 0.36393586 0.36255571 0.36228137 0.36265141 0.36293431 0.35917974\n",
      " 0.36134371 0.36193999 0.36291526 0.36086854 0.36483424 0.36224775\n",
      " 0.36336969 0.36294399 0.36020187 0.36491676 0.36450537 0.36084501\n",
      " 0.36205304 0.36171522 0.36280499 0.3619526  0.36187073 0.3636791\n",
      " 0.36111321 0.3650249  0.36406005 0.36324803 0.36320272 0.36365328\n",
      " 0.36365324 0.36342511 0.36653526 0.36561128 0.36555615 0.36416106\n",
      " 0.36613416 0.36716734 0.36487564 0.3644509  0.36430889 0.36486188\n",
      " 0.36305695 0.36833104 0.36806674 0.36567496 0.36310903 0.36603263\n",
      " 0.36376138 0.36389474 0.365752   0.36648887 0.36333158 0.36629765\n",
      " 0.36941746 0.36888745 0.36660074 0.36694989 0.36797701 0.36349125\n",
      " 0.36900527 0.3677102  0.36534472 0.36969945 0.36962124 0.3699115\n",
      " 0.36681582 0.36576763 0.36563133 0.36641368 0.36565442 0.36557734\n",
      " 0.36884731 0.36635436 0.36677586 0.36789007 0.36913081 0.36592418\n",
      " 0.36857183 0.36912366 0.36731568 0.36311126 0.36719095 0.36503981\n",
      " 0.36968527 0.3649816  0.36707572 0.36599987 0.36651433 0.37099579\n",
      " 0.36996243 0.36877234 0.36758892 0.36501013 0.36562145 0.36807794\n",
      " 0.36520487]\n",
      "losses sum: 39.797108633126\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.3651110883773028....\n",
      "\n",
      "Epoch 4/10\n",
      "length of data is 330\n",
      "lossses: [0.3645564  0.36726398 0.36537989 0.36783866 0.36602403 0.37075169\n",
      " 0.36433995 0.36831569 0.36794523 0.36289032 0.36047215 0.36385282\n",
      " 0.36558838 0.36385156 0.36222666 0.36269261 0.36419955 0.35682418\n",
      " 0.36304264 0.36122668 0.36225471 0.36191052 0.36837772 0.3628048\n",
      " 0.36599118 0.36191478 0.35641734 0.36533086 0.36597718 0.35846481\n",
      " 0.3588446  0.35769775 0.36092979 0.35881496 0.35814286 0.36285087\n",
      " 0.35524675 0.36256219 0.35996638 0.35901539 0.3569441  0.35806366\n",
      " 0.36274785 0.36167601 0.36191707 0.36145958 0.35618005 0.35561763\n",
      " 0.35939171 0.36213997 0.35241574 0.35517106 0.35332108 0.35741363\n",
      " 0.34874311 0.35648534 0.3614409  0.35668018 0.3539229  0.35407623\n",
      " 0.34740255 0.35087295 0.35466453 0.354573   0.34786737 0.35015976\n",
      " 0.35687095 0.35507394 0.35276197 0.35386684 0.35512964 0.34264385\n",
      " 0.35667663 0.34895161 0.35188679 0.3518811  0.35539124 0.35738388\n",
      " 0.34631091 0.34780146 0.34678345 0.34705025 0.34281048 0.3447364\n",
      " 0.34834138 0.34996572 0.34513588 0.35336332 0.34962157 0.34156772\n",
      " 0.34838379 0.3527105  0.3470681  0.34183803 0.34727748 0.34449644\n",
      " 0.34853464 0.34130623 0.34512915 0.3395764  0.34224076 0.3515434\n",
      " 0.34739904 0.34809597 0.34187576 0.33999279 0.34244993 0.34672078\n",
      " 0.33957934]\n",
      "losses sum: 38.72237199136844\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.3552511191868664....\n",
      "\n",
      "Epoch 5/10\n",
      "length of data is 440\n",
      "lossses: [0.33898035 0.34530694 0.34313427 0.3465331  0.3424686  0.3469467\n",
      " 0.3397138  0.34420279 0.34400757 0.33932724 0.33148103 0.3338927\n",
      " 0.340268   0.33913676 0.33694769 0.3351839  0.33935715 0.32608599\n",
      " 0.34478841 0.33554993 0.33492138 0.34300385 0.34876215 0.33854404\n",
      " 0.34619708 0.33813166 0.33118696 0.33943171 0.34332446 0.3389107\n",
      " 0.33362008 0.33399767 0.3418685  0.33322133 0.33724382 0.34102584\n",
      " 0.32950628 0.34473315 0.33669949 0.33779384 0.33380452 0.33465456\n",
      " 0.34352678 0.34855782 0.34060574 0.34436087 0.33159788 0.33387644\n",
      " 0.3391006  0.34784078 0.3333971  0.33643012 0.33038633 0.34531534\n",
      " 0.33012951 0.3405896  0.34995123 0.34226491 0.33837915 0.33385197\n",
      " 0.32995568 0.33443696 0.34105274 0.34344356 0.32925545 0.3304558\n",
      " 0.34337012 0.34276999 0.33807886 0.34108617 0.34552491 0.32462946\n",
      " 0.34575539 0.33242046 0.34509359 0.33690898 0.34655479 0.35595701\n",
      " 0.33446391 0.33733002 0.33404661 0.33561828 0.33047966 0.33004818\n",
      " 0.34022531 0.33753972 0.33836109 0.34521997 0.34313105 0.32642254\n",
      " 0.3374734  0.34741894 0.3390318  0.33294109 0.33883152 0.33725907\n",
      " 0.34207583 0.33298714 0.33924051 0.32953206 0.33395617 0.3456836\n",
      " 0.33834721 0.34406501 0.33428809 0.33410381 0.33603089 0.34365965\n",
      " 0.33817573]\n",
      "losses sum: 36.90879594593961\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.33861280684348266....\n",
      "\n",
      "Epoch 6/10\n",
      "length of data is 550\n",
      "lossses: [0.33484031 0.34380384 0.33869943 0.3469264  0.33961167 0.34640992\n",
      " 0.33682937 0.34548181 0.34444099 0.33827071 0.32918246 0.32913032\n",
      " 0.34035193 0.33521481 0.33832461 0.33272276 0.33997859 0.32136883\n",
      " 0.34488972 0.33347892 0.33309845 0.34172345 0.35051846 0.33969691\n",
      " 0.34705229 0.3390239  0.33246733 0.34055457 0.34537653 0.34149695\n",
      " 0.33486218 0.3352132  0.34458655 0.33311392 0.34136915 0.34053983\n",
      " 0.32884869 0.35095818 0.33754434 0.33775768 0.33835643 0.33732648\n",
      " 0.34095804 0.34977679 0.34094771 0.35048946 0.33486633 0.33490981\n",
      " 0.34175977 0.35313237 0.33777802 0.33975203 0.32972653 0.35008778\n",
      " 0.33353666 0.34769371 0.35693711 0.3452785  0.33982198 0.33387601\n",
      " 0.33488234 0.33762607 0.34276927 0.35139717 0.3302122  0.33462551\n",
      " 0.34909495 0.34884425 0.34224413 0.34593969 0.35197767 0.32822803\n",
      " 0.35229077 0.33638701 0.3520979  0.3412504  0.35329513 0.36534988\n",
      " 0.34298523 0.34082666 0.336735   0.3408472  0.33646259 0.33419774\n",
      " 0.34714527 0.34057819 0.34838546 0.35055563 0.35044479 0.33070992\n",
      " 0.3415297  0.35594461 0.34561052 0.33933697 0.34321336 0.34133553\n",
      " 0.35001696 0.33966198 0.34712326 0.3350579  0.34195057 0.35311098\n",
      " 0.34355029 0.35173322 0.3433219  0.3434067  0.34181687 0.35408528\n",
      " 0.34822569]\n",
      "losses sum: 37.251189796003686\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.34175403482572186....\n",
      "\n",
      "Epoch 7/10\n",
      "length of data is 660\n",
      "lossses: [0.34194852 0.35420932 0.34541883 0.3571149  0.34818494 0.35863325\n",
      " 0.34570209 0.35774668 0.35654674 0.34780743 0.33772948 0.33683107\n",
      " 0.3512484  0.34344688 0.34977489 0.34152921 0.35067219 0.32779769\n",
      " 0.35360236 0.34093173 0.34158391 0.34708808 0.36197791 0.3499008\n",
      " 0.35647735 0.34936882 0.34145932 0.35130272 0.35659649 0.35054109\n",
      " 0.34648991 0.34378121 0.35336568 0.34179168 0.35199183 0.34869981\n",
      " 0.3354222  0.36280028 0.34666611 0.34552973 0.34983054 0.34820598\n",
      " 0.34727697 0.35403483 0.34809891 0.36263452 0.34703966 0.34452397\n",
      " 0.35158956 0.36460206 0.34945179 0.3497471  0.33703303 0.35963271\n",
      " 0.34477264 0.36196085 0.36899897 0.35412639 0.34885156 0.34213777\n",
      " 0.34443542 0.34579878 0.3499584  0.36350232 0.33859986 0.34561557\n",
      " 0.36030999 0.35966703 0.35214466 0.35683477 0.36356644 0.33663286\n",
      " 0.36493896 0.34741695 0.36322483 0.35305289 0.3651749  0.37737303\n",
      " 0.35574486 0.34845407 0.34326575 0.35141448 0.34699444 0.34524243\n",
      " 0.35881995 0.3503034  0.36167628 0.36060213 0.3623225  0.34146253\n",
      " 0.35177735 0.36820092 0.35804115 0.35018252 0.35204407 0.35019741\n",
      " 0.36330681 0.35157614 0.36001009 0.34608943 0.35428947 0.36518048\n",
      " 0.35329937 0.36276701 0.3560663  0.35640468 0.35248286 0.36735602\n",
      " 0.35978761]\n",
      "losses sum: 38.35787153026396\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.35190707825930234....\n",
      "\n",
      "Epoch 8/10\n",
      "length of data is 770\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lossses: [0.35335464 0.36715533 0.35509435 0.37102786 0.36054417 0.37409797\n",
      " 0.35761003 0.37406679 0.37159828 0.36026026 0.34882779 0.34865554\n",
      " 0.36522066 0.35502916 0.36362536 0.35346997 0.36375873 0.33810876\n",
      " 0.36576035 0.35174474 0.35331319 0.35593421 0.37650653 0.36389855\n",
      " 0.36863666 0.36311156 0.35319497 0.36550321 0.37044101 0.361357\n",
      " 0.36156898 0.35510402 0.36440475 0.35354905 0.36506275 0.3604804\n",
      " 0.34500411 0.37681625 0.36034585 0.35667622 0.36320091 0.3622243\n",
      " 0.35766954 0.36164421 0.35753531 0.37667594 0.36174323 0.35770082\n",
      " 0.36461093 0.37731744 0.36228783 0.36182183 0.34854732 0.37115666\n",
      " 0.35834676 0.37718865 0.38285065 0.36579099 0.36032283 0.3537325\n",
      " 0.35585277 0.35651465 0.35997479 0.37657961 0.35029299 0.3589128\n",
      " 0.37384645 0.37173133 0.36477859 0.37020239 0.37717452 0.34630991\n",
      " 0.3794116  0.35991365 0.37610284 0.36821231 0.37911886 0.39114108\n",
      " 0.36939961 0.35847385 0.3523844  0.3642342  0.36006933 0.3590561\n",
      " 0.37267106 0.36232619 0.37594514 0.37326992 0.37666941 0.35450068\n",
      " 0.36448239 0.38161073 0.37226821 0.36214017 0.36347381 0.36133624\n",
      " 0.37937392 0.36564535 0.37447024 0.36010424 0.36844323 0.37836671\n",
      " 0.36487584 0.37550952 0.37008572 0.37096864 0.36500556 0.38183546\n",
      " 0.37200482]\n",
      "losses sum: 39.73936047413136\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.3645812887534987....\n",
      "\n",
      "Epoch 9/10\n",
      "length of data is 880\n",
      "lossses: [0.36651158 0.38066031 0.36607862 0.38688846 0.37419689 0.39039535\n",
      " 0.37061153 0.39253404 0.3877752  0.37396583 0.36051094 0.36302272\n",
      " 0.38032252 0.36776282 0.37795783 0.36747565 0.37729347 0.35060071\n",
      " 0.37961435 0.36423652 0.36744895 0.36657372 0.39267348 0.37973341\n",
      " 0.38170284 0.37869219 0.36607659 0.38144855 0.38567748 0.37320632\n",
      " 0.37784235 0.3680001  0.37733001 0.36620992 0.37916162 0.37443085\n",
      " 0.35588986 0.39142333 0.37712004 0.36975134 0.37749134 0.37715658\n",
      " 0.37028771 0.37212118 0.36900493 0.39218719 0.37687785 0.37266978\n",
      " 0.37959664 0.39153995 0.37586774 0.37572661 0.36158077 0.3842309\n",
      " 0.37263768 0.39268812 0.39730916 0.37982798 0.37308049 0.36664327\n",
      " 0.36861489 0.36891911 0.37140946 0.38982425 0.36364968 0.37356489\n",
      " 0.38861633 0.38455235 0.37848333 0.38521629 0.39113578 0.35663206\n",
      " 0.39472841 0.37242991 0.3903817  0.38486618 0.3941152  0.40588153\n",
      " 0.38297392 0.37053416 0.36331061 0.37767279 0.37447641 0.37422358\n",
      " 0.3880136  0.37547118 0.39070609 0.38727649 0.39241947 0.36894595\n",
      " 0.37835839 0.39507012 0.38626556 0.37416018 0.37630906 0.37387131\n",
      " 0.39702609 0.38066135 0.3892421  0.37578041 0.38399852 0.39169978\n",
      " 0.37804954 0.38981961 0.38529867 0.38723121 0.37887242 0.39693421\n",
      " 0.38436887]\n",
      "losses sum: 41.25939717522519\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.3785265795892219....\n",
      "\n",
      "Epoch 10/10\n",
      "length of data is 990\n",
      "lossses: [0.38011432 0.39381444 0.37812512 0.40364779 0.38854946 0.40734266\n",
      " 0.38443238 0.4110604  0.40417378 0.38874671 0.37302117 0.37848024\n",
      " 0.39592561 0.38102483 0.39273926 0.38261363 0.39133473 0.36520326\n",
      " 0.39531803 0.37784543 0.38290938 0.37920412 0.41076966 0.39640816\n",
      " 0.39577066 0.3962831  0.38007914 0.39882061 0.4025392  0.38669025\n",
      " 0.3944416  0.38219782 0.3925999  0.37902925 0.39420801 0.39010634\n",
      " 0.36795247 0.40660953 0.39475075 0.38351042 0.39279484 0.39288785\n",
      " 0.38451233 0.38460172 0.38236759 0.4100996  0.39227038 0.38882435\n",
      " 0.39569347 0.40777981 0.39022659 0.39127629 0.37506765 0.39873629\n",
      " 0.38719626 0.40910951 0.41292517 0.39541914 0.38682414 0.38044279\n",
      " 0.3825467  0.38250619 0.38432888 0.40373123 0.37839811 0.38896872\n",
      " 0.40383483 0.39835509 0.3926599  0.40146543 0.40656854 0.3677557\n",
      " 0.41183189 0.38615066 0.40560187 0.40229971 0.41011272 0.42120113\n",
      " 0.39782483 0.38332176 0.37577514 0.39238349 0.38975715 0.38947968\n",
      " 0.40445874 0.38890085 0.40614024 0.4019313  0.40989162 0.38450928\n",
      " 0.39398652 0.40900383 0.40167093 0.38585185 0.38945792 0.388229\n",
      " 0.41585532 0.39664705 0.40442809 0.3919492  0.40114053 0.40586607\n",
      " 0.39362009 0.4055095  0.40044912 0.40623291 0.39443086 0.41194054\n",
      " 0.39782173]\n",
      "losses sum: 42.8942318107204\n",
      "losses length: 109\n",
      "\n",
      "loss: 0.39352506248367336....\n"
     ]
    }
   ],
   "source": [
    "\n",
    "NN = NeuralNetwork(encoded_words, lr=0.5)\n",
    "embedding = layer(len(encoded_words),13, \"word_embedding\", activation= \"tanh\")\n",
    "hidden_layer = layer(13,5, \"hidden_layer\", activation=\"relu\")\n",
    "output_layer = layer(5,1, \"output_layer\", activation=\"sigmoid\")\n",
    "NN.add(embedding)\n",
    "NN.add(hidden_layer)\n",
    "NN.add(output_layer)\n",
    "\n",
    "\n",
    "NN.fit(X_train,y_train, batch_size=32, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "f9fe1740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = NN.predict(np.array(X_test.iloc[0:]))\n",
    "y_pred= np.where(y_pred<0.5,0,1)[0]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "fe3ddbb5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.84      0.83     16018\n",
      "         1.0       0.77      0.73      0.75     11094\n",
      "\n",
      "    accuracy                           0.80     27112\n",
      "   macro avg       0.79      0.79      0.79     27112\n",
      "weighted avg       0.80      0.80      0.80     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test[0:], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c400c1",
   "metadata": {},
   "source": [
    "# Exporting the model using Joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "0dd50af6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./nn_model_1.joblib']"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(NN, \"./nn_model_1.joblib\", compress=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
