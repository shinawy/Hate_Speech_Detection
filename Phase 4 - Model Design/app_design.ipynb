{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f389942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.python.client import device_lib\n",
    "# from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import cuml.LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0447004f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>hatespeech</th>\n",
       "      <th>text</th>\n",
       "      <th>preprocess_data</th>\n",
       "      <th>word_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yes indeed. She sort of reminds me of the elde...</td>\n",
       "      <td>['yes', 'indeed', 'sort', 'remind', 'eld', 'la...</td>\n",
       "      <td>{'yes': 1, 'indeed': 1, 'sort': 1, 'remind': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The trans women reading this tweet right now i...</td>\n",
       "      <td>['trans', 'woman', 'read', 'tweet', 'right', '...</td>\n",
       "      <td>{'trans': 1, 'woman': 1, 'read': 1, 'tweet': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Question: These 4 broads who criticize America...</td>\n",
       "      <td>['question', 'broad', 'criticize', 'america', ...</td>\n",
       "      <td>{'question': 1, 'broad': 1, 'criticize': 1, 'a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It is about time for all illegals to go back t...</td>\n",
       "      <td>['time', 'illegal', 'go', 'back', 'country', '...</td>\n",
       "      <td>{'time': 1, 'illegal': 1, 'go': 1, 'back': 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>For starters bend over the one in pink and kic...</td>\n",
       "      <td>['starter', 'bend', 'one', 'pink', 'kick', 'as...</td>\n",
       "      <td>{'starter': 1, 'bend': 1, 'one': 1, 'pink': 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135551</th>\n",
       "      <td>135551</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ÿπÿßÿ¨ŸÑ ÿ≥ŸÖÿßÿ≠ÿ© #ÿßŸÑÿ≥ŸäÿØ_ÿπÿ®ÿØÿßŸÑŸÖŸÑŸÉ_ÿ®ÿØÿ±ÿßŸÑÿØŸäŸÜ_ÿßŸÑÿ≠Ÿàÿ´Ÿä  ŸÜÿµ...</td>\n",
       "      <td>['break', 'news', 'sayye', 'abdulmalik', 'saud...</td>\n",
       "      <td>{'break': 1, 'news': 1, 'sayye': 1, 'abdulmali...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135552</th>\n",
       "      <td>135552</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Millions of #Yemen-is participated in mass ral...</td>\n",
       "      <td>['million', 'yemen', 'participate', 'mass', 'r...</td>\n",
       "      <td>{'million': 1, 'yemen': 1, 'participate': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135553</th>\n",
       "      <td>135553</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@AbeShinzo @realDonaldTrump @shinzoabe Áã¨Ë£ÅËÄÖ„ÅØË°å„Åç„Åæ...</td>\n",
       "      <td>['dictator', 'go', 'people', 'iran', 'stay', '...</td>\n",
       "      <td>{'dictator': 1, 'go': 1, 'people': 1, 'iran': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135554</th>\n",
       "      <td>135554</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Millions of #Yemen-is participated in mass ral...</td>\n",
       "      <td>['million', 'yemen', 'participate', 'mass', 'r...</td>\n",
       "      <td>{'million': 1, 'yemen': 1, 'participate': 1, '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135555</th>\n",
       "      <td>135555</td>\n",
       "      <td>1.0</td>\n",
       "      <td>ŸÑÿß ÿ™ÿ™ÿ¥ŸÖÿ™ ÿßŸÑÿ±ÿ¨ÿßŸÑ ŸÖÿ≥ŸÉŸäŸÜ ŸäÿπÿßŸÜŸä ŸÉÿ≥ ÿßŸÖŸá üòÇ. ŸäŸÇŸàŸÑ ŸäÿßŸÑ...</td>\n",
       "      <td>['op', 'really', 'hope', 'commit', 'suicide', ...</td>\n",
       "      <td>{'op': 1, 'really': 1, 'hope': 1, 'commit': 1,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135556 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0  hatespeech  \\\n",
       "0                0         0.0   \n",
       "1                1         0.0   \n",
       "2                2         1.0   \n",
       "3                3         0.0   \n",
       "4                4         1.0   \n",
       "...            ...         ...   \n",
       "135551      135551         0.0   \n",
       "135552      135552         0.0   \n",
       "135553      135553         0.0   \n",
       "135554      135554         0.0   \n",
       "135555      135555         1.0   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Yes indeed. She sort of reminds me of the elde...   \n",
       "1       The trans women reading this tweet right now i...   \n",
       "2       Question: These 4 broads who criticize America...   \n",
       "3       It is about time for all illegals to go back t...   \n",
       "4       For starters bend over the one in pink and kic...   \n",
       "...                                                   ...   \n",
       "135551  ÿπÿßÿ¨ŸÑ ÿ≥ŸÖÿßÿ≠ÿ© #ÿßŸÑÿ≥ŸäÿØ_ÿπÿ®ÿØÿßŸÑŸÖŸÑŸÉ_ÿ®ÿØÿ±ÿßŸÑÿØŸäŸÜ_ÿßŸÑÿ≠Ÿàÿ´Ÿä  ŸÜÿµ...   \n",
       "135552  Millions of #Yemen-is participated in mass ral...   \n",
       "135553  @AbeShinzo @realDonaldTrump @shinzoabe Áã¨Ë£ÅËÄÖ„ÅØË°å„Åç„Åæ...   \n",
       "135554  Millions of #Yemen-is participated in mass ral...   \n",
       "135555  ŸÑÿß ÿ™ÿ™ÿ¥ŸÖÿ™ ÿßŸÑÿ±ÿ¨ÿßŸÑ ŸÖÿ≥ŸÉŸäŸÜ ŸäÿπÿßŸÜŸä ŸÉÿ≥ ÿßŸÖŸá üòÇ. ŸäŸÇŸàŸÑ ŸäÿßŸÑ...   \n",
       "\n",
       "                                          preprocess_data  \\\n",
       "0       ['yes', 'indeed', 'sort', 'remind', 'eld', 'la...   \n",
       "1       ['trans', 'woman', 'read', 'tweet', 'right', '...   \n",
       "2       ['question', 'broad', 'criticize', 'america', ...   \n",
       "3       ['time', 'illegal', 'go', 'back', 'country', '...   \n",
       "4       ['starter', 'bend', 'one', 'pink', 'kick', 'as...   \n",
       "...                                                   ...   \n",
       "135551  ['break', 'news', 'sayye', 'abdulmalik', 'saud...   \n",
       "135552  ['million', 'yemen', 'participate', 'mass', 'r...   \n",
       "135553  ['dictator', 'go', 'people', 'iran', 'stay', '...   \n",
       "135554  ['million', 'yemen', 'participate', 'mass', 'r...   \n",
       "135555  ['op', 'really', 'hope', 'commit', 'suicide', ...   \n",
       "\n",
       "                                               word_tfidf  \n",
       "0       {'yes': 1, 'indeed': 1, 'sort': 1, 'remind': 1...  \n",
       "1       {'trans': 1, 'woman': 1, 'read': 1, 'tweet': 1...  \n",
       "2       {'question': 1, 'broad': 1, 'criticize': 1, 'a...  \n",
       "3       {'time': 1, 'illegal': 1, 'go': 1, 'back': 1, ...  \n",
       "4       {'starter': 1, 'bend': 1, 'one': 1, 'pink': 1,...  \n",
       "...                                                   ...  \n",
       "135551  {'break': 1, 'news': 1, 'sayye': 1, 'abdulmali...  \n",
       "135552  {'million': 1, 'yemen': 1, 'participate': 1, '...  \n",
       "135553  {'dictator': 1, 'go': 1, 'people': 1, 'iran': ...  \n",
       "135554  {'million': 1, 'yemen': 1, 'participate': 1, '...  \n",
       "135555  {'op': 1, 'really': 1, 'hope': 1, 'commit': 1,...  \n",
       "\n",
       "[135556 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df= pd.read_csv(\"./final_cleaned_data.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f86ef3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.preprocess_data, df.hatespeech, test_size = 0.20, random_state = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c54b3",
   "metadata": {},
   "source": [
    "# Loading the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9308d720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.392504</td>\n",
       "      <td>0.096169</td>\n",
       "      <td>0.162693</td>\n",
       "      <td>0.156266</td>\n",
       "      <td>0.156319</td>\n",
       "      <td>-0.099668</td>\n",
       "      <td>0.100789</td>\n",
       "      <td>0.290475</td>\n",
       "      <td>0.308128</td>\n",
       "      <td>0.103893</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053987</td>\n",
       "      <td>0.044126</td>\n",
       "      <td>0.482509</td>\n",
       "      <td>0.239502</td>\n",
       "      <td>0.240314</td>\n",
       "      <td>-0.258498</td>\n",
       "      <td>0.241423</td>\n",
       "      <td>-0.369123</td>\n",
       "      <td>-0.258710</td>\n",
       "      <td>-0.416328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.033204</td>\n",
       "      <td>0.261176</td>\n",
       "      <td>0.089114</td>\n",
       "      <td>-0.278039</td>\n",
       "      <td>0.298973</td>\n",
       "      <td>-0.130887</td>\n",
       "      <td>-0.065391</td>\n",
       "      <td>-0.054846</td>\n",
       "      <td>0.334370</td>\n",
       "      <td>0.229851</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.165243</td>\n",
       "      <td>-0.315131</td>\n",
       "      <td>0.288015</td>\n",
       "      <td>-0.073173</td>\n",
       "      <td>0.307313</td>\n",
       "      <td>0.067266</td>\n",
       "      <td>-0.098658</td>\n",
       "      <td>-0.244250</td>\n",
       "      <td>-0.226284</td>\n",
       "      <td>-0.354271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.453792</td>\n",
       "      <td>0.144635</td>\n",
       "      <td>0.236152</td>\n",
       "      <td>0.393797</td>\n",
       "      <td>-0.145903</td>\n",
       "      <td>-0.257484</td>\n",
       "      <td>-0.249625</td>\n",
       "      <td>-0.129302</td>\n",
       "      <td>0.193659</td>\n",
       "      <td>-0.283933</td>\n",
       "      <td>...</td>\n",
       "      <td>0.300729</td>\n",
       "      <td>-0.212626</td>\n",
       "      <td>0.303604</td>\n",
       "      <td>0.068350</td>\n",
       "      <td>0.186978</td>\n",
       "      <td>0.231174</td>\n",
       "      <td>0.029016</td>\n",
       "      <td>-0.235607</td>\n",
       "      <td>-0.028078</td>\n",
       "      <td>-0.287045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.162786</td>\n",
       "      <td>0.252368</td>\n",
       "      <td>0.112504</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.100086</td>\n",
       "      <td>-0.149958</td>\n",
       "      <td>0.151108</td>\n",
       "      <td>0.215548</td>\n",
       "      <td>-0.150607</td>\n",
       "      <td>0.012614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.224296</td>\n",
       "      <td>-0.137284</td>\n",
       "      <td>0.095827</td>\n",
       "      <td>0.010119</td>\n",
       "      <td>0.002221</td>\n",
       "      <td>-0.075656</td>\n",
       "      <td>-0.016011</td>\n",
       "      <td>-0.110389</td>\n",
       "      <td>-0.105161</td>\n",
       "      <td>-0.151208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.124603</td>\n",
       "      <td>0.930885</td>\n",
       "      <td>0.662424</td>\n",
       "      <td>0.596124</td>\n",
       "      <td>0.118784</td>\n",
       "      <td>-0.582222</td>\n",
       "      <td>0.516050</td>\n",
       "      <td>0.509593</td>\n",
       "      <td>0.891478</td>\n",
       "      <td>-0.115511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.543330</td>\n",
       "      <td>-0.250377</td>\n",
       "      <td>0.159628</td>\n",
       "      <td>0.169906</td>\n",
       "      <td>0.128468</td>\n",
       "      <td>0.252294</td>\n",
       "      <td>-0.192518</td>\n",
       "      <td>-0.054293</td>\n",
       "      <td>-0.079778</td>\n",
       "      <td>-0.294656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108439</th>\n",
       "      <td>-0.069671</td>\n",
       "      <td>0.581804</td>\n",
       "      <td>0.373661</td>\n",
       "      <td>-0.059654</td>\n",
       "      <td>-0.014828</td>\n",
       "      <td>0.076886</td>\n",
       "      <td>0.098548</td>\n",
       "      <td>0.175721</td>\n",
       "      <td>-0.305804</td>\n",
       "      <td>0.116572</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.240525</td>\n",
       "      <td>-0.169877</td>\n",
       "      <td>0.674411</td>\n",
       "      <td>0.286542</td>\n",
       "      <td>-0.166273</td>\n",
       "      <td>0.025956</td>\n",
       "      <td>-0.284985</td>\n",
       "      <td>-0.474955</td>\n",
       "      <td>-0.211929</td>\n",
       "      <td>-0.433161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108440</th>\n",
       "      <td>0.187033</td>\n",
       "      <td>-0.255897</td>\n",
       "      <td>0.176649</td>\n",
       "      <td>0.022109</td>\n",
       "      <td>-0.143560</td>\n",
       "      <td>0.018096</td>\n",
       "      <td>0.178578</td>\n",
       "      <td>0.222256</td>\n",
       "      <td>0.598812</td>\n",
       "      <td>-0.269883</td>\n",
       "      <td>...</td>\n",
       "      <td>0.286556</td>\n",
       "      <td>-0.072101</td>\n",
       "      <td>0.553434</td>\n",
       "      <td>0.407236</td>\n",
       "      <td>-0.158145</td>\n",
       "      <td>0.258417</td>\n",
       "      <td>-0.208022</td>\n",
       "      <td>-0.037641</td>\n",
       "      <td>-0.183402</td>\n",
       "      <td>-0.249320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108441</th>\n",
       "      <td>0.274607</td>\n",
       "      <td>0.326903</td>\n",
       "      <td>0.365647</td>\n",
       "      <td>-0.291814</td>\n",
       "      <td>-0.090395</td>\n",
       "      <td>0.044838</td>\n",
       "      <td>-0.032316</td>\n",
       "      <td>-0.005742</td>\n",
       "      <td>0.348736</td>\n",
       "      <td>-0.141631</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.188262</td>\n",
       "      <td>-0.223386</td>\n",
       "      <td>0.514807</td>\n",
       "      <td>0.343326</td>\n",
       "      <td>0.112395</td>\n",
       "      <td>0.247604</td>\n",
       "      <td>-0.155018</td>\n",
       "      <td>-0.397081</td>\n",
       "      <td>-0.437747</td>\n",
       "      <td>-0.508698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108442</th>\n",
       "      <td>-0.017247</td>\n",
       "      <td>0.075185</td>\n",
       "      <td>0.368429</td>\n",
       "      <td>-0.206547</td>\n",
       "      <td>0.051130</td>\n",
       "      <td>-0.006052</td>\n",
       "      <td>0.220270</td>\n",
       "      <td>0.198259</td>\n",
       "      <td>0.158434</td>\n",
       "      <td>-0.203625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.089837</td>\n",
       "      <td>-0.255830</td>\n",
       "      <td>0.623477</td>\n",
       "      <td>0.328603</td>\n",
       "      <td>-0.056488</td>\n",
       "      <td>0.229703</td>\n",
       "      <td>0.024259</td>\n",
       "      <td>-0.016266</td>\n",
       "      <td>-0.157488</td>\n",
       "      <td>-0.206090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108443</th>\n",
       "      <td>-0.002077</td>\n",
       "      <td>0.036132</td>\n",
       "      <td>0.086293</td>\n",
       "      <td>-0.197850</td>\n",
       "      <td>0.443603</td>\n",
       "      <td>-0.063369</td>\n",
       "      <td>-0.118852</td>\n",
       "      <td>0.162129</td>\n",
       "      <td>0.275412</td>\n",
       "      <td>-0.235112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.203699</td>\n",
       "      <td>0.038620</td>\n",
       "      <td>-0.111003</td>\n",
       "      <td>0.135991</td>\n",
       "      <td>-0.083858</td>\n",
       "      <td>0.237915</td>\n",
       "      <td>0.375709</td>\n",
       "      <td>-0.058036</td>\n",
       "      <td>-0.182937</td>\n",
       "      <td>-0.120272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>108444 rows √ó 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.392504  0.096169  0.162693  0.156266  0.156319 -0.099668  0.100789   \n",
       "1      -0.033204  0.261176  0.089114 -0.278039  0.298973 -0.130887 -0.065391   \n",
       "2       0.453792  0.144635  0.236152  0.393797 -0.145903 -0.257484 -0.249625   \n",
       "3      -0.162786  0.252368  0.112504  0.010795  0.100086 -0.149958  0.151108   \n",
       "4       0.124603  0.930885  0.662424  0.596124  0.118784 -0.582222  0.516050   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "108439 -0.069671  0.581804  0.373661 -0.059654 -0.014828  0.076886  0.098548   \n",
       "108440  0.187033 -0.255897  0.176649  0.022109 -0.143560  0.018096  0.178578   \n",
       "108441  0.274607  0.326903  0.365647 -0.291814 -0.090395  0.044838 -0.032316   \n",
       "108442 -0.017247  0.075185  0.368429 -0.206547  0.051130 -0.006052  0.220270   \n",
       "108443 -0.002077  0.036132  0.086293 -0.197850  0.443603 -0.063369 -0.118852   \n",
       "\n",
       "               7         8         9  ...       490       491       492  \\\n",
       "0       0.290475  0.308128  0.103893  ... -0.053987  0.044126  0.482509   \n",
       "1      -0.054846  0.334370  0.229851  ... -0.165243 -0.315131  0.288015   \n",
       "2      -0.129302  0.193659 -0.283933  ...  0.300729 -0.212626  0.303604   \n",
       "3       0.215548 -0.150607  0.012614  ... -0.224296 -0.137284  0.095827   \n",
       "4       0.509593  0.891478 -0.115511  ...  0.543330 -0.250377  0.159628   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "108439  0.175721 -0.305804  0.116572  ... -0.240525 -0.169877  0.674411   \n",
       "108440  0.222256  0.598812 -0.269883  ...  0.286556 -0.072101  0.553434   \n",
       "108441 -0.005742  0.348736 -0.141631  ... -0.188262 -0.223386  0.514807   \n",
       "108442  0.198259  0.158434 -0.203625  ...  0.089837 -0.255830  0.623477   \n",
       "108443  0.162129  0.275412 -0.235112  ...  0.203699  0.038620 -0.111003   \n",
       "\n",
       "             493       494       495       496       497       498       499  \n",
       "0       0.239502  0.240314 -0.258498  0.241423 -0.369123 -0.258710 -0.416328  \n",
       "1      -0.073173  0.307313  0.067266 -0.098658 -0.244250 -0.226284 -0.354271  \n",
       "2       0.068350  0.186978  0.231174  0.029016 -0.235607 -0.028078 -0.287045  \n",
       "3       0.010119  0.002221 -0.075656 -0.016011 -0.110389 -0.105161 -0.151208  \n",
       "4       0.169906  0.128468  0.252294 -0.192518 -0.054293 -0.079778 -0.294656  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "108439  0.286542 -0.166273  0.025956 -0.284985 -0.474955 -0.211929 -0.433161  \n",
       "108440  0.407236 -0.158145  0.258417 -0.208022 -0.037641 -0.183402 -0.249320  \n",
       "108441  0.343326  0.112395  0.247604 -0.155018 -0.397081 -0.437747 -0.508698  \n",
       "108442  0.328603 -0.056488  0.229703  0.024259 -0.016266 -0.157488 -0.206090  \n",
       "108443  0.135991 -0.083858  0.237915  0.375709 -0.058036 -0.182937 -0.120272  \n",
       "\n",
       "[108444 rows x 500 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df= pd.read_csv(\"word2vec_train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65421dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>490</th>\n",
       "      <th>491</th>\n",
       "      <th>492</th>\n",
       "      <th>493</th>\n",
       "      <th>494</th>\n",
       "      <th>495</th>\n",
       "      <th>496</th>\n",
       "      <th>497</th>\n",
       "      <th>498</th>\n",
       "      <th>499</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.027884</td>\n",
       "      <td>0.154510</td>\n",
       "      <td>0.133212</td>\n",
       "      <td>-0.048337</td>\n",
       "      <td>-0.023450</td>\n",
       "      <td>-0.023614</td>\n",
       "      <td>0.155422</td>\n",
       "      <td>0.214377</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>-0.080604</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080944</td>\n",
       "      <td>-0.109479</td>\n",
       "      <td>0.188402</td>\n",
       "      <td>0.060850</td>\n",
       "      <td>0.128112</td>\n",
       "      <td>0.015127</td>\n",
       "      <td>-0.120649</td>\n",
       "      <td>-0.173245</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.016392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.107833</td>\n",
       "      <td>0.136457</td>\n",
       "      <td>0.144665</td>\n",
       "      <td>-0.080500</td>\n",
       "      <td>0.115311</td>\n",
       "      <td>-0.027881</td>\n",
       "      <td>0.110125</td>\n",
       "      <td>-0.033345</td>\n",
       "      <td>0.024141</td>\n",
       "      <td>-0.179432</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.181468</td>\n",
       "      <td>-0.172876</td>\n",
       "      <td>0.152576</td>\n",
       "      <td>-0.079823</td>\n",
       "      <td>0.107255</td>\n",
       "      <td>0.046340</td>\n",
       "      <td>-0.033024</td>\n",
       "      <td>-0.221537</td>\n",
       "      <td>-0.153597</td>\n",
       "      <td>-0.275859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.187029</td>\n",
       "      <td>0.089364</td>\n",
       "      <td>-0.062768</td>\n",
       "      <td>-0.073457</td>\n",
       "      <td>0.109764</td>\n",
       "      <td>-0.002750</td>\n",
       "      <td>-0.193237</td>\n",
       "      <td>-0.255880</td>\n",
       "      <td>0.098945</td>\n",
       "      <td>0.145007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.314949</td>\n",
       "      <td>-0.163641</td>\n",
       "      <td>0.309937</td>\n",
       "      <td>-0.470525</td>\n",
       "      <td>0.539774</td>\n",
       "      <td>-0.286998</td>\n",
       "      <td>0.193061</td>\n",
       "      <td>-0.537482</td>\n",
       "      <td>-0.272638</td>\n",
       "      <td>-0.330768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.417618</td>\n",
       "      <td>-0.145028</td>\n",
       "      <td>0.234264</td>\n",
       "      <td>0.108180</td>\n",
       "      <td>-0.099731</td>\n",
       "      <td>-0.089281</td>\n",
       "      <td>0.167048</td>\n",
       "      <td>-0.061286</td>\n",
       "      <td>0.337373</td>\n",
       "      <td>-0.146726</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307296</td>\n",
       "      <td>-0.054767</td>\n",
       "      <td>0.561383</td>\n",
       "      <td>0.315499</td>\n",
       "      <td>0.017259</td>\n",
       "      <td>0.119335</td>\n",
       "      <td>-0.015660</td>\n",
       "      <td>-0.001797</td>\n",
       "      <td>-0.093725</td>\n",
       "      <td>-0.158892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.048772</td>\n",
       "      <td>0.482660</td>\n",
       "      <td>0.664313</td>\n",
       "      <td>0.296277</td>\n",
       "      <td>0.250464</td>\n",
       "      <td>-0.600320</td>\n",
       "      <td>0.120753</td>\n",
       "      <td>0.605743</td>\n",
       "      <td>0.210446</td>\n",
       "      <td>0.095419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307695</td>\n",
       "      <td>-0.377147</td>\n",
       "      <td>0.172046</td>\n",
       "      <td>0.581602</td>\n",
       "      <td>0.278406</td>\n",
       "      <td>0.242657</td>\n",
       "      <td>0.201805</td>\n",
       "      <td>-0.015586</td>\n",
       "      <td>-0.605102</td>\n",
       "      <td>-0.013302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27107</th>\n",
       "      <td>-0.004341</td>\n",
       "      <td>0.386919</td>\n",
       "      <td>0.619110</td>\n",
       "      <td>0.194278</td>\n",
       "      <td>0.095667</td>\n",
       "      <td>-0.507506</td>\n",
       "      <td>0.572744</td>\n",
       "      <td>0.425896</td>\n",
       "      <td>0.077603</td>\n",
       "      <td>-0.525112</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010876</td>\n",
       "      <td>0.059631</td>\n",
       "      <td>-0.039927</td>\n",
       "      <td>-0.027920</td>\n",
       "      <td>-0.461081</td>\n",
       "      <td>0.154524</td>\n",
       "      <td>0.187492</td>\n",
       "      <td>-0.114061</td>\n",
       "      <td>-0.230649</td>\n",
       "      <td>-0.442103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27108</th>\n",
       "      <td>0.105398</td>\n",
       "      <td>-0.050238</td>\n",
       "      <td>0.393533</td>\n",
       "      <td>-0.157628</td>\n",
       "      <td>-0.102172</td>\n",
       "      <td>-0.031130</td>\n",
       "      <td>-0.007233</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>0.461258</td>\n",
       "      <td>-0.335302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138597</td>\n",
       "      <td>-0.116039</td>\n",
       "      <td>0.482112</td>\n",
       "      <td>0.329050</td>\n",
       "      <td>0.045887</td>\n",
       "      <td>0.318437</td>\n",
       "      <td>0.241486</td>\n",
       "      <td>-0.147374</td>\n",
       "      <td>-0.212067</td>\n",
       "      <td>-0.353587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27109</th>\n",
       "      <td>0.237704</td>\n",
       "      <td>0.222205</td>\n",
       "      <td>0.242136</td>\n",
       "      <td>-0.352349</td>\n",
       "      <td>0.104166</td>\n",
       "      <td>-0.068679</td>\n",
       "      <td>0.034842</td>\n",
       "      <td>0.041449</td>\n",
       "      <td>0.130387</td>\n",
       "      <td>-0.063410</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.155173</td>\n",
       "      <td>-0.449139</td>\n",
       "      <td>0.467923</td>\n",
       "      <td>-0.166822</td>\n",
       "      <td>0.264839</td>\n",
       "      <td>0.054631</td>\n",
       "      <td>0.096024</td>\n",
       "      <td>-0.398065</td>\n",
       "      <td>-0.189992</td>\n",
       "      <td>-0.297972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27110</th>\n",
       "      <td>-0.368356</td>\n",
       "      <td>0.524395</td>\n",
       "      <td>0.154059</td>\n",
       "      <td>-0.070640</td>\n",
       "      <td>0.135439</td>\n",
       "      <td>-0.065014</td>\n",
       "      <td>0.307924</td>\n",
       "      <td>0.071840</td>\n",
       "      <td>-0.001720</td>\n",
       "      <td>-0.284981</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311698</td>\n",
       "      <td>-0.104643</td>\n",
       "      <td>0.029650</td>\n",
       "      <td>-0.073186</td>\n",
       "      <td>0.001547</td>\n",
       "      <td>0.168698</td>\n",
       "      <td>-0.042282</td>\n",
       "      <td>-0.152808</td>\n",
       "      <td>-0.057375</td>\n",
       "      <td>-0.342395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27111</th>\n",
       "      <td>0.230347</td>\n",
       "      <td>0.661390</td>\n",
       "      <td>0.670011</td>\n",
       "      <td>0.625876</td>\n",
       "      <td>-0.166977</td>\n",
       "      <td>-0.515434</td>\n",
       "      <td>-0.103361</td>\n",
       "      <td>0.460190</td>\n",
       "      <td>0.446700</td>\n",
       "      <td>-0.414965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808327</td>\n",
       "      <td>-0.000853</td>\n",
       "      <td>0.125521</td>\n",
       "      <td>0.595031</td>\n",
       "      <td>-0.278579</td>\n",
       "      <td>0.554891</td>\n",
       "      <td>-0.446738</td>\n",
       "      <td>-0.247130</td>\n",
       "      <td>0.089830</td>\n",
       "      <td>-0.040038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27112 rows √ó 500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0     -0.027884  0.154510  0.133212 -0.048337 -0.023450 -0.023614  0.155422   \n",
       "1     -0.107833  0.136457  0.144665 -0.080500  0.115311 -0.027881  0.110125   \n",
       "2      0.187029  0.089364 -0.062768 -0.073457  0.109764 -0.002750 -0.193237   \n",
       "3      0.417618 -0.145028  0.234264  0.108180 -0.099731 -0.089281  0.167048   \n",
       "4     -0.048772  0.482660  0.664313  0.296277  0.250464 -0.600320  0.120753   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "27107 -0.004341  0.386919  0.619110  0.194278  0.095667 -0.507506  0.572744   \n",
       "27108  0.105398 -0.050238  0.393533 -0.157628 -0.102172 -0.031130 -0.007233   \n",
       "27109  0.237704  0.222205  0.242136 -0.352349  0.104166 -0.068679  0.034842   \n",
       "27110 -0.368356  0.524395  0.154059 -0.070640  0.135439 -0.065014  0.307924   \n",
       "27111  0.230347  0.661390  0.670011  0.625876 -0.166977 -0.515434 -0.103361   \n",
       "\n",
       "              7         8         9  ...       490       491       492  \\\n",
       "0      0.214377  0.004131 -0.080604  ... -0.080944 -0.109479  0.188402   \n",
       "1     -0.033345  0.024141 -0.179432  ... -0.181468 -0.172876  0.152576   \n",
       "2     -0.255880  0.098945  0.145007  ... -0.314949 -0.163641  0.309937   \n",
       "3     -0.061286  0.337373 -0.146726  ...  0.307296 -0.054767  0.561383   \n",
       "4      0.605743  0.210446  0.095419  ...  0.307695 -0.377147  0.172046   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "27107  0.425896  0.077603 -0.525112  ... -0.010876  0.059631 -0.039927   \n",
       "27108  0.007230  0.461258 -0.335302  ...  0.138597 -0.116039  0.482112   \n",
       "27109  0.041449  0.130387 -0.063410  ... -0.155173 -0.449139  0.467923   \n",
       "27110  0.071840 -0.001720 -0.284981  ... -0.311698 -0.104643  0.029650   \n",
       "27111  0.460190  0.446700 -0.414965  ...  0.808327 -0.000853  0.125521   \n",
       "\n",
       "            493       494       495       496       497       498       499  \n",
       "0      0.060850  0.128112  0.015127 -0.120649 -0.173245 -0.180167 -0.016392  \n",
       "1     -0.079823  0.107255  0.046340 -0.033024 -0.221537 -0.153597 -0.275859  \n",
       "2     -0.470525  0.539774 -0.286998  0.193061 -0.537482 -0.272638 -0.330768  \n",
       "3      0.315499  0.017259  0.119335 -0.015660 -0.001797 -0.093725 -0.158892  \n",
       "4      0.581602  0.278406  0.242657  0.201805 -0.015586 -0.605102 -0.013302  \n",
       "...         ...       ...       ...       ...       ...       ...       ...  \n",
       "27107 -0.027920 -0.461081  0.154524  0.187492 -0.114061 -0.230649 -0.442103  \n",
       "27108  0.329050  0.045887  0.318437  0.241486 -0.147374 -0.212067 -0.353587  \n",
       "27109 -0.166822  0.264839  0.054631  0.096024 -0.398065 -0.189992 -0.297972  \n",
       "27110 -0.073186  0.001547  0.168698 -0.042282 -0.152808 -0.057375 -0.342395  \n",
       "27111  0.595031 -0.278579  0.554891 -0.446738 -0.247130  0.089830 -0.040038  \n",
       "\n",
       "[27112 rows x 500 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df= pd.read_csv(\"word2vec_test.csv\")\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3523d6ac",
   "metadata": {},
   "source": [
    "# Neural Network (Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e2eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_logistic = MLPClassifier(hidden_layer_sizes=(25,),random_state=1, max_iter=5000, activation='logistic')\n",
    "nn_model_logistic.fit(train_df.values,y_train.values)\n",
    "nn_results_logistic = nn_model_logistic.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa37dc70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.83      0.83      0.83     16018\n",
      "         1.0       0.75      0.75      0.75     11094\n",
      "\n",
      "    accuracy                           0.79     27112\n",
      "   macro avg       0.79      0.79      0.79     27112\n",
      "weighted avg       0.79      0.79      0.79     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_logistic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "908ccbd7",
   "metadata": {},
   "source": [
    "# Neural Network Relu (Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "addf6aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_relu = MLPClassifier(hidden_layer_sizes=(25,),random_state=1, max_iter=5000, activation='relu')\n",
    "nn_model_relu.fit(train_df.values,y_train.values)\n",
    "nn_results_relu = nn_model_relu.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e9a05a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.85      0.83     16018\n",
      "         1.0       0.77      0.72      0.74     11094\n",
      "\n",
      "    accuracy                           0.80     27112\n",
      "   macro avg       0.79      0.78      0.79     27112\n",
      "weighted avg       0.80      0.80      0.80     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_relu))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91931f6b",
   "metadata": {},
   "source": [
    "# Neural Network Relu lbfgs(Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "031b9ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cse/.conda/envs/gpunew/lib/python3.6/site-packages/sklearn/neural_network/_multilayer_perceptron.py:500: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_relulbfgs = MLPClassifier(hidden_layer_sizes=(25,),random_state=1,solver='lbfgs', max_iter=5000, activation='relu')\n",
    "nn_model_relulbfgs.fit(train_df.values,y_train.values)\n",
    "nn_results_relulbfgs = nn_model_relulbfgs.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1f55b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.84      0.83     16018\n",
      "         1.0       0.76      0.72      0.74     11094\n",
      "\n",
      "    accuracy                           0.79     27112\n",
      "   macro avg       0.79      0.78      0.78     27112\n",
      "weighted avg       0.79      0.79      0.79     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_relulbfgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b67b047",
   "metadata": {},
   "source": [
    "# Neural Network Relu sgd (Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b09aa8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_relusgd = MLPClassifier(hidden_layer_sizes=(25,),random_state=1, solver= 'sgd',max_iter=5000, activation='relu')\n",
    "nn_model_relusgd.fit(train_df.values,y_train.values)\n",
    "nn_results_relusgd = nn_model_relusgd.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb84a57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.85      0.83     16018\n",
      "         1.0       0.77      0.72      0.74     11094\n",
      "\n",
      "    accuracy                           0.80     27112\n",
      "   macro avg       0.79      0.78      0.79     27112\n",
      "weighted avg       0.79      0.80      0.79     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_relusgd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3767e9",
   "metadata": {},
   "source": [
    "# Neural Network Relu adam (Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6cec9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_reluadam = MLPClassifier(hidden_layer_sizes=(25,),random_state=1,solver='adam', max_iter=5000, activation='relu')\n",
    "nn_model_reluadam.fit(train_df.values,y_train.values)\n",
    "nn_results_reluadam = nn_model_reluadam.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bedfbce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.85      0.83     16018\n",
      "         1.0       0.77      0.72      0.74     11094\n",
      "\n",
      "    accuracy                           0.80     27112\n",
      "   macro avg       0.79      0.78      0.79     27112\n",
      "weighted avg       0.80      0.80      0.80     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_reluadam))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17cbf59",
   "metadata": {},
   "source": [
    "# Neural Network Relu adam 0.01 Learning Rate (Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f0f6bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_reluadam01 = MLPClassifier(hidden_layer_sizes=(25,),random_state=1,solver='adam',learning_rate_init= 0.01,max_iter=5000, activation='relu')\n",
    "nn_model_reluadam01.fit(train_df.values,y_train.values)\n",
    "nn_results_reluadam01 = nn_model_reluadam01.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8dc91cd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.86      0.83     16018\n",
      "         1.0       0.78      0.70      0.74     11094\n",
      "\n",
      "    accuracy                           0.80     27112\n",
      "   macro avg       0.79      0.78      0.78     27112\n",
      "weighted avg       0.79      0.80      0.79     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_reluadam01))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebe4bd2",
   "metadata": {},
   "source": [
    "# Neural Network Relu adam 0.001(Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1348844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_reluadam001 = MLPClassifier(hidden_layer_sizes=(25,),random_state=1,solver='adam',learning_rate_init=0.001, max_iter=5000, activation='relu')\n",
    "nn_model_reluadam001.fit(train_df.values,y_train.values)\n",
    "nn_results_reluadam001 = nn_model_reluadam001.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "106095ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.85      0.83     16018\n",
      "         1.0       0.77      0.72      0.74     11094\n",
      "\n",
      "    accuracy                           0.80     27112\n",
      "   macro avg       0.79      0.78      0.79     27112\n",
      "weighted avg       0.80      0.80      0.80     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_reluadam001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965c68cc",
   "metadata": {},
   "source": [
    "# Neural Network Relu adam 0.0001 (Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b968492",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_reluadam0001 = MLPClassifier(hidden_layer_sizes=(25,),random_state=1,solver='adam',learning_rate_init=0.0001, max_iter=5000, activation='relu')\n",
    "nn_model_reluadam0001.fit(train_df.values,y_train.values)\n",
    "nn_results_reluadam0001 = nn_model_reluadam0001.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64c3a1b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.86      0.83     16018\n",
      "         1.0       0.77      0.71      0.74     11094\n",
      "\n",
      "    accuracy                           0.80     27112\n",
      "   macro avg       0.79      0.78      0.79     27112\n",
      "weighted avg       0.80      0.80      0.80     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_reluadam0001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc221b6c",
   "metadata": {},
   "source": [
    "# Neural Network Relu adam 0.00001 (Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0d35a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_reluadam00001 = MLPClassifier(hidden_layer_sizes=(25,),random_state=1,solver='adam',learning_rate_init=0.00001, max_iter=5000, activation='relu')\n",
    "nn_model_reluadam00001.fit(train_df.values,y_train.values)\n",
    "nn_results_reluadam00001 = nn_model_reluadam00001.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55fb1c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.85      0.83     16018\n",
      "         1.0       0.77      0.71      0.73     11094\n",
      "\n",
      "    accuracy                           0.79     27112\n",
      "   macro avg       0.79      0.78      0.78     27112\n",
      "weighted avg       0.79      0.79      0.79     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_reluadam00001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8112400f",
   "metadata": {},
   "source": [
    "# Neural Network Relu adam 0.0001 adaptive (Hidden Layer= 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "463cfc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "nn_model_reluadam0001 = MLPClassifier(hidden_layer_sizes=(25,),random_state=1,solver='adam',learning_rate='adaptive',learning_rate_init=0.0001, max_iter=5000, activation='relu')\n",
    "nn_model_reluadam0001.fit(train_df.values,y_train.values)\n",
    "nn_results_reluadam0001 = nn_model_reluadam0001.predict(test_df.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "375f2b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      0.86      0.83     16018\n",
      "         1.0       0.77      0.71      0.74     11094\n",
      "\n",
      "    accuracy                           0.80     27112\n",
      "   macro avg       0.79      0.78      0.79     27112\n",
      "weighted avg       0.80      0.80      0.80     27112\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "print(classification_report(y_test.values,nn_results_reluadam0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31e6a1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpunew",
   "language": "python",
   "name": "gpunew"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
